{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "#### <<< Warning suppression>>> ###\n",
    "# import warnings\n",
    "# warnings.filterwarnings('deprecated')\n",
    "#### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time as timepackage\n",
    "\n",
    "from numpy import newaxis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "\n",
    "from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "from utils_eval             import c_index, brier_score\n",
    "from utils_log              import save_logging, load_logging\n",
    "from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    \"\"\"\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    \"\"\"\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all\n",
    "\n",
    "## cmd args: \n",
    "# now only one argument is needed\n",
    "# this will be something like \"PreCar\"\n",
    "# and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the following codes read model training results plus needed data from Model_Training.py\n",
    "# and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "'''\n",
    "saver.restore(sess, sys.argv[1])\n",
    "with open(sys.argv[2]) as p: \n",
    "    params = json.load(p)\n",
    "'''\n",
    "\n",
    "# argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "# argv[2], if left empty, will choose the most recent log\n",
    "# if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "sys.argv = ['mod', 'PreCar', '807445_my_aMAP_model_with_CNVs','1', '6', '0.01', '1']\n",
    "data_mode_name = sys.argv[1]\n",
    "\n",
    "if len(sys.argv) < 7: \n",
    "    # this means no argv[2] is given; we use the most recent log\n",
    "    # to do so, for now lets just use max argument\n",
    "    # firstly, take out all log.json documents\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    # logs is a list of all available logs; find the most recent one...\n",
    "    target_dir = data_mode_name + '/' + max(logs)\n",
    "    print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "else: \n",
    "    # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    matched = [i for i in logs if sys.argv[2] in i]\n",
    "    if len(matched) >= 2: \n",
    "        print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "        matched = max(matched)\n",
    "    target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "# read log\n",
    "with open(target_dir + '/' + '_log.json') as p: \n",
    "    params = json.load(p)\n",
    "mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "# print(type(params))\n",
    "new_parser = params['new_parser']\n",
    "dataset_info = params['dataset_info']\n",
    "evaluation_info = params['evaluation_info']\n",
    "model_configs = params['model_configs']\n",
    "eval_configs = params['eval_configs']\n",
    "time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "dirs = dataset_info\n",
    "test_dir = []\n",
    "data_mode = data_mode_name\n",
    "for key in list(dirs.keys()): \n",
    "    if key == data_mode: \n",
    "        train_dir = dirs[key]\n",
    "    else: \n",
    "        test_dir.append(dirs[key])\n",
    "\n",
    "(tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little treat: print name (in dict) of dataset\n",
    "def get_key(val):\n",
    "    for key, value in dataset_info.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"There is no such Key\"\n",
    "\n",
    "train_name = get_key(train_dir)\n",
    "test1_name = get_key(test_dir[0])\n",
    "test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : tf.nn.relu,\n",
    "                                'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                 }\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, mod_dir)\n",
    "\n",
    "# By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "# Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "risk_all = f_get_risk_predictions(sess, model, te_data, te_data_mi, pred_time, eval_time)\n",
    "\n",
    "for p, p_time in enumerate(pred_time):\n",
    "    pred_horizon = int(p_time)\n",
    "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "    for t, t_time in enumerate(eval_time):                \n",
    "        eval_horizon = int(t_time) + pred_horizon\n",
    "        for k in range(num_Event):\n",
    "            result1[k, t] = c_index(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            result2[k, t] = brier_score(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    \n",
    "    if p == 0:\n",
    "        final1, final2 = result1, result2\n",
    "    else:\n",
    "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "        \n",
    "        \n",
    "row_header = []\n",
    "for p_time in pred_time:\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "            \n",
    "col_header = []\n",
    "for t_time in eval_time:\n",
    "    col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "# c-index result\n",
    "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# brier-score result\n",
    "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "### PRINT RESULTS\n",
    "print('========================================================')\n",
    "print('Test set external validation')\n",
    "print('Data: ' + test1_name)\n",
    "print('========================================================')\n",
    "print('--------------------------------------------------------')\n",
    "print('- C-INDEX: ')\n",
    "print(df1)\n",
    "print('--------------------------------------------------------')\n",
    "print('- BRIER-SCORE: ')\n",
    "print(df2)\n",
    "print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6230288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "prange = list(range(4))\n",
    "erange = list(range(5))\n",
    "\n",
    "# create figures directory\n",
    "fig_dir = target_dir + '/eval/figures/'\n",
    "\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "\n",
    "for pidx in prange: \n",
    "    for eidx in erange: \n",
    "        \n",
    "        # firstly, if there is no event in this range, the programme will return with error\n",
    "        # check with df1\n",
    "        if df1.iloc[pidx, eidx] == 0: \n",
    "            this_pred_time = pred_time[pidx]\n",
    "            this_eval_time = eval_time[eidx]\n",
    "            print('There is no event during the following timeframe: [' + str(this_eval_time) + \", \" + str(this_eval_time + this_pred_time) + \"], therefore no discrimination could be made and no KM plot could be produced in this way. \")\n",
    "        else: \n",
    "            \n",
    "            risk = risk_all[0]\n",
    "\n",
    "            risk = risk[:, pidx, eidx]\n",
    "            this_pred_time = pred_time[pidx]\n",
    "            this_eval_time = eval_time[eidx]\n",
    "\n",
    "            fig_name = 'Landmark_' + str(this_eval_time) + '_Horizon_' + str(this_pred_time) + '_KMP.png'\n",
    "            print('Plotting ' + fig_name)\n",
    "\n",
    "            # identify pops who are still susceptible\n",
    "            eval_horizon = this_eval_time\n",
    "            idx = [i > eval_horizon for i in list(te_time[:, 0])]\n",
    "            te_data_sub = te_data[idx, :, :]\n",
    "\n",
    "            # for these patients, their risks\n",
    "            risk_sub = risk[idx]\n",
    "\n",
    "            # patient id and time\n",
    "            te_time_sub = te_time[idx, 0]\n",
    "            te_label_sub = te_label[idx, 0]\n",
    "\n",
    "            # check how many patients left\n",
    "            # print('Time-varying sample size: ' + str(risk_sub.shape[0]))\n",
    "\n",
    "            risk_max = max(risk_sub)\n",
    "            risk_min = min(risk_sub)\n",
    "\n",
    "            # construct a range of risks\n",
    "            step_size = (risk_max - risk_min)/100\n",
    "            steps = [risk_min + (i + 1) * step_size for i in range(100-1)]\n",
    "\n",
    "            # this is a good 100-sized step, checked\n",
    "\n",
    "            # now develop that into a for loop...\n",
    "            pvalL = []\n",
    "\n",
    "            for step in steps: \n",
    "\n",
    "\n",
    "\n",
    "                # divide pops based on the step\n",
    "                grp1_idx = [i > step for i in risk_sub]\n",
    "                grp0_idx = [i <= step for i in risk_sub]\n",
    "\n",
    "                grp1_data = te_data_sub[grp1_idx, :, :]\n",
    "                grp1_time = te_time_sub[grp1_idx]\n",
    "                grp1_label = te_label_sub[grp1_idx]\n",
    "\n",
    "                # new label that is time-dynamic\n",
    "                grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "                grp1_label_new = np.zeros(len(grp1_label))\n",
    "                grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "                grp0_data = te_data_sub[grp0_idx, :, :]\n",
    "                grp0_time = te_time_sub[grp0_idx]\n",
    "                grp0_label = te_label_sub[grp0_idx]\n",
    "\n",
    "                grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "                grp0_label_new = np.zeros(len(grp0_label))\n",
    "                grp0_label_new[grp0_label_idx] = 1\n",
    "\n",
    "\n",
    "                # now KM\n",
    "                lrt = logrank_test(grp0_time, grp1_time, grp0_label_new, grp1_label_new)\n",
    "                # print p value, just to check\n",
    "                # print('p value: ' + str(lrt.p_value))\n",
    "                # log p value\n",
    "                pvalL.append(lrt.p_value)\n",
    "\n",
    "            min_p = min(pvalL)\n",
    "            min_p_idx = [i for i in range(len(steps)) if pvalL[i] == min_p][0]\n",
    "            min_p_steps = steps[min_p_idx]\n",
    "\n",
    "            # let us use steps[50]\n",
    "            step = steps[min_p_idx]\n",
    "\n",
    "            # divide pop into two risk \n",
    "            # identify pops who are still susceptible\n",
    "            eval_horizon = this_eval_time\n",
    "            idx = [i > eval_horizon for i in list(te_time[:, 0])]\n",
    "            te_data_sub = te_data[idx, :, :]\n",
    "\n",
    "            # for these patients, their risks\n",
    "            risk_sub = risk[idx]\n",
    "\n",
    "            # patient id and time\n",
    "            te_time_sub = te_time[idx, 0]\n",
    "            te_label_sub = te_label[idx, 0]\n",
    "\n",
    "            # divide pops based on the step\n",
    "            grp1_idx = [i > step for i in risk_sub]\n",
    "            grp0_idx = [i <= step for i in risk_sub]\n",
    "\n",
    "            grp1_data = te_data_sub[grp1_idx, :, :]\n",
    "            grp1_time = te_time_sub[grp1_idx]\n",
    "            grp1_label = te_label_sub[grp1_idx]\n",
    "            # apply a mask\n",
    "            # for i in range(len(grp1_label)): \n",
    "                # grp1_label[i] = \n",
    "\n",
    "            grp0_data = te_data_sub[grp0_idx, :, :]\n",
    "            grp0_time = te_time_sub[grp0_idx]\n",
    "            grp0_label = te_label_sub[grp0_idx]\n",
    "\n",
    "\n",
    "            # now KM\n",
    "            # lrt = logrank_test(grp0_time, grp1_time, grp0_label, grp1_label)\n",
    "            # here, K-M curve fitting\n",
    "\n",
    "            # first param is grpx_time, second param is grpx_label\n",
    "\n",
    "            # reset the plotting canvas\n",
    "            plt.figure()\n",
    "\n",
    "            window_lb = this_eval_time\n",
    "            if window_lb == 1: \n",
    "                window_lb = 0\n",
    "            print(window_lb)\n",
    "            window_up = this_eval_time + this_pred_time\n",
    "            print(window_up)\n",
    "            kmf1 = KaplanMeierFitter()\n",
    "            kmf1.fit(grp0_time, grp0_label, label = \"Low_risk\")\n",
    "            p0 = kmf1.plot_cumulative_density(loc = slice(window_lb, window_up))\n",
    "\n",
    "            kmf1.fit(grp1_time, grp1_label, label = \"High_risk\")\n",
    "            p1 = kmf1.plot_cumulative_density(ax = p0, loc = slice(window_lb, window_up))\n",
    "            pfin = p1.get_figure()\n",
    "            plt.title('Landmark: ' + str(this_eval_time) + ', Horizon: ' + str(this_pred_time) + ', p-value: ' + str('%.3e'%min_p) + ', c-index: ' + str('%.3f'%df1.iloc[pidx, eidx]))\n",
    "            fig_name = 'Landmark_' + str(this_eval_time) + '_Horizon_' + str(this_pred_time) + '_KMP.png'\n",
    "            pfin.savefig(target_dir + '/eval/figures/' + fig_name)\n",
    "            del p0\n",
    "            del p1 \n",
    "            del pfin\n",
    "            del kmf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e38cf058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.776'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str('%.3f'%df1.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3b96c9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[2, 4] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d54e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
