{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6afd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script simulates the process of increasing sample size in improving model precision\n",
    "\n",
    "# although I myself think the idea was terrible...\n",
    "\n",
    "\n",
    "\n",
    "_EPSILON = 1e-08\n",
    "\n",
    "\n",
    "\n",
    "from cmath import inf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time as timepackage\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "\n",
    "from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "from utils_eval             import c_index, brier_score\n",
    "from utils_log              import save_logging, load_logging\n",
    "from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "sys.argv = ['xx', './params/onlyBL.json']\n",
    "#time_tag = timepackage.strftime(\"%d_%b_%Y_%H%M%S%f_GMT\", sys_time)\n",
    "time_tag = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S-%f')\n",
    "\n",
    "# read in external parameters passed from the console\n",
    "with open(sys.argv[1]) as p: \n",
    "    params = json.load(p)\n",
    "new_parser = params[\"new_parser\"]\n",
    "dataset_info = params[\"dataset_info\"]\n",
    "evaluation_info = params[\"evaluation_info\"]\n",
    "model_configs = params[\"model_configs\"]\n",
    "eval_configs = params[\"eval_configs\"]\n",
    "\n",
    "params[\"new_parser\"][\"time_tag\"] = time_tag\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def recursion(dic): \n",
    "    for key, value in dic.items(): \n",
    "        if isinstance(value, np.floating): \n",
    "            dic[key] = float(value)\n",
    "        elif isinstance(value, dict): \n",
    "            recursion(value)\n",
    "\n",
    "\n",
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    \"\"\"\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    \"\"\"\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all\n",
    "\n",
    "\n",
    "# ### 1. Import Dataset\n",
    "# #####      - Users must prepare dataset in csv format and modify \"import_data.py\" following our examplar \"PBC2\"\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "data_mode                   = model_configs[\"data_mode\"]\n",
    "seed                        = model_configs[\"seed\"]\n",
    "\n",
    "##### IMPORT DATASET\n",
    "\"\"\"\n",
    "    num_Category            = max event/censoring time * 1.2\n",
    "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
    "    max_length              = maximum number of measurements\n",
    "    x_dim                   = data dimension including delta (1 + num_features)\n",
    "    x_dim_cont              = dim of continuous features\n",
    "    x_dim_bin               = dim of binary features\n",
    "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
    "\"\"\"\n",
    "dirs = dataset_info\n",
    "test_dir = []\n",
    "for key in list(dirs.keys()): \n",
    "    if key == data_mode: \n",
    "        train_dir = dirs[key]\n",
    "    else: \n",
    "        test_dir.append(dirs[key])\n",
    "    \n",
    "\n",
    "(tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs[\"bin_list\"], cont_list_in = model_configs[\"cont_list\"], log_list = model_configs[\"log_transform\"])\n",
    "\n",
    "(te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs[\"bin_list\"], cont_list_in = model_configs[\"cont_list\"], log_list = model_configs[\"log_transform\"])\n",
    "\n",
    "(tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs[\"bin_list\"], cont_list_in = model_configs[\"cont_list\"], log_list = model_configs[\"log_transform\"])\n",
    "\n",
    "pred_time = evaluation_info[\"pred_time\"] # prediction time (in months)\n",
    "eval_time = evaluation_info[\"eval_time\"] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "\n",
    "mode_path = \"{}\".format(data_mode)\n",
    "\n",
    "if not os.path.exists(mode_path):\n",
    "    os.makedirs(mode_path)\n",
    "\n",
    "file_path = mode_path + \"/\" + time_tag + \"_\" + new_parser[\"reference\"]\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "\n",
    "# ### 2. Set Hyper-Parameters\n",
    "# ##### - Play with your own hyper-parameters!\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "burn_in_mode                = model_configs[\"burnin_mode\"]\n",
    "boost_mode                  = model_configs[\"boost_mode\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# INPUT DIMENSIONS\n",
    "input_dims                  = { \"x_dim\"         : tr_x_dim,\n",
    "                                \"x_dim_cont\"    : tr_x_dim_cont,\n",
    "                                \"x_dim_bin\"     : tr_x_dim_bin,\n",
    "                                \"num_Event\"     : num_Event,\n",
    "                                \"num_Category\"  : num_Category,\n",
    "                                \"max_length\"    : max_length }\n",
    "\n",
    "# NETWORK HYPER-PARMETERS\n",
    "network_settings            = { \"h_dim_RNN\"         : new_parser[\"h_dim_RNN\"],\n",
    "                                \"h_dim_FC\"          : new_parser[\"h_dim_FC\"],\n",
    "                                \"num_layers_RNN\"    : new_parser[\"num_layers_RNN\"],\n",
    "                                \"num_layers_ATT\"    : new_parser[\"num_layers_ATT\"],\n",
    "                                \"num_layers_CS\"     : new_parser[\"num_layers_CS\"],\n",
    "                                \"RNN_type\"          : new_parser[\"RNN_type\"],\n",
    "                                \"FC_active_fn\"      : tf.nn.relu,\n",
    "                                \"RNN_active_fn\"     : tf.nn.tanh,\n",
    "                                \"initial_W\"         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                \"reg_W\"             : new_parser[\"reg_W\"],\n",
    "                                \"reg_W_out\"         : float(new_parser[\"reg_W_out\"])\n",
    "                                 }\n",
    "\n",
    "\n",
    "mb_size           = new_parser[\"mb_size\"]\n",
    "iteration         = new_parser[\"iteration\"]\n",
    "iteration_burn_in = new_parser[\"iteration_burn_in\"]\n",
    "\n",
    "keep_prob         = new_parser[\"keep_prob\"]\n",
    "lr_train          = new_parser[\"lr_train\"]\n",
    "\n",
    "alpha             = new_parser[\"alpha\"]\n",
    "beta              = new_parser[\"beta\"]\n",
    "gamma             = new_parser[\"gamma\"]\n",
    "\n",
    "# SAVE HYPERPARAMETERS\n",
    "log_name = file_path + \"/\" + \"_log.json\"\n",
    "log_file = params\n",
    "\n",
    "recursion(log_file)\n",
    "\n",
    "with open(log_name, \"w\") as f:\n",
    "    json.dump(log_file, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c2654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2148, 1, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 3. Split Dataset into Train/Valid/Test Sets\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "(tr_data,va_data, tr_data_mi, va_data_mi, tr_time,va_time, tr_label,va_label, \n",
    " tr_mask1,va_mask1, tr_mask2,va_mask2, tr_mask3,va_mask3) = train_test_split(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3, test_size=model_configs[\"val_ratio\"], random_state = seed) \n",
    "\n",
    "if boost_mode == \"ON\":\n",
    "    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
