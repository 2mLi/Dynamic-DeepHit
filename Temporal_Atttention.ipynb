{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434e6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-05-12_07-26-34-665087_my_aMAP_model_with_CNVs/model\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[node save/Assign_7 (defined at C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py:230) ]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n    app.start()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n    return runner(coro)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py\", line 230, in <module>\n    saver = tf.train.Saver()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in __init__\n    self.build()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 63, in assign\n    use_locking=use_locking, name=name)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[node save/Assign_7 (defined at C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py:230) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[{{node save/Assign_7}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1276\u001b[1;33m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1277\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[node save/Assign_7 (defined at C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py:230) ]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n    app.start()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n    return runner(coro)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py\", line 230, in <module>\n    saver = tf.train.Saver()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in __init__\n    self.build()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 63, in assign\n    use_locking=use_locking, name=name)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[node save/Assign_7 (defined at C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py:230) ]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[1;31m# We add a more reasonable error message here to help users (b/110263146)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       raise _wrap_restore_error_with_msg(\n\u001b[1;32m-> 1312\u001b[1;33m           err, \"a mismatch between the current graph and the graph\")\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[node save/Assign_7 (defined at C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py:230) ]]\n\nCaused by op 'save/Assign_7', defined at:\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n    app.start()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n    self.io_loop.start()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n    await self.process_one()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n    await dispatch(*args)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n    await result\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n    reply_content = await reply_content\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n    return runner(coro)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py\", line 230, in <module>\n    saver = tf.train.Saver()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 832, in __init__\n    self.build()\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 354, in _AddRestoreOps\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\", line 73, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 223, in assign\n    validate_shape=validate_shape)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 63, in assign\n    use_locking=use_locking, name=name)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nAssign requires shapes of both tensors to match. lhs shape= [100,42] rhs shape= [100,39]\n\t [[node save/Assign_7 (defined at C:\\Users\\lizhm5766\\AppData\\Local\\Temp\\ipykernel_12044\\2642092660.py:230) ]]\n"
     ]
    }
   ],
   "source": [
    "_EPSILON = 1e-08\n",
    "\n",
    "#### <<< Warning suppression>>> ###\n",
    "# import warnings\n",
    "# warnings.filterwarnings('deprecated')\n",
    "#### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time as timepackage\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "\n",
    "from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "from utils_eval             import c_index, brier_score\n",
    "from utils_log              import save_logging, load_logging\n",
    "from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    \"\"\"\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    \"\"\"\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all\n",
    "\n",
    "## cmd args: \n",
    "# now only one argument is needed\n",
    "# this will be something like \"PreCar\"\n",
    "# and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the following codes read model training results plus needed data from Model_Training.py\n",
    "# and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "'''\n",
    "saver.restore(sess, sys.argv[1])\n",
    "with open(sys.argv[2]) as p: \n",
    "    params = json.load(p)\n",
    "'''\n",
    "sys.argv = ['.py', 'PreCar', '665087']\n",
    "# argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "# argv[2], if left empty, will choose the most recent log\n",
    "# if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "data_mode_name = sys.argv[1]\n",
    "\n",
    "if len(sys.argv) < 3: \n",
    "    # this means no argv[2] is given; we use the most recent log\n",
    "    # to do so, for now lets just use max argument\n",
    "    # firstly, take out all log.json documents\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    # logs is a list of all available logs; find the most recent one...\n",
    "    target_dir = data_mode_name + '/' + max(logs)\n",
    "    print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "else: \n",
    "    # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    matched = [i for i in logs if sys.argv[2] in i]\n",
    "    if len(matched) >= 2: \n",
    "        print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "        matched = max(matched)\n",
    "    target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "# read log\n",
    "with open(target_dir + '/' + '_log.json') as p: \n",
    "    params = json.load(p)\n",
    "mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "# print(type(params))\n",
    "new_parser = params['new_parser']\n",
    "dataset_info = params['dataset_info']\n",
    "evaluation_info = params['evaluation_info']\n",
    "model_configs = params['model_configs']\n",
    "eval_configs = params['eval_configs']\n",
    "time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "dirs = dataset_info\n",
    "test_dir = []\n",
    "data_mode = data_mode_name\n",
    "for key in list(dirs.keys()): \n",
    "    if key == data_mode: \n",
    "        train_dir = dirs[key]\n",
    "    else: \n",
    "        test_dir.append(dirs[key])\n",
    "\n",
    "(tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "# check whether the dimension of tr_data, te_data and tea_data match\n",
    "# check for second dimension... \n",
    "if tr_data.shape[1] > te_data.shape[1] : \n",
    "    # this means te_data have fewer follow-ups than tr_data. For this, patch it up with vectors of zero. \n",
    "    print('Test set [1] has fewer follow-ups than train set. Artificially generated follow-ups have been attached. ')\n",
    "    k = tr_data.shape[1] - te_data.shape[1]\n",
    "    for i in range(k): \n",
    "        te_data = np.append(te_data, np.zeros(shape = (te_data.shape[0], 1, te_data.shape[2]), dtype = float), axis = 1) \n",
    "        te_data_mi = np.append(te_data_mi, np.zeros(shape = (te_data_mi.shape[0], 1, te_data_mi.shape[2]), dtype = float), axis = 1) \n",
    "\n",
    "if tr_data.shape[1] > tea_data.shape[1] : \n",
    "    \n",
    "    print('Test set [2] has fewer follow-ups than train set. Artificially generated follow-ups have been attached. ')\n",
    "    k = tr_data.shape[1] - tea_data.shape[1]\n",
    "    for i in range(k): \n",
    "        tea_data = np.append(tea_data, np.zeros(shape = (tea_data.shape[0], 1, tea_data.shape[2]), dtype = float), axis = 1) \n",
    "        tea_data_mi = np.append(tea_data_mi, np.zeros(shape = (tea_data_mi.shape[0], 1, tea_data_mi.shape[2]), dtype = float), axis = 1) \n",
    "\n",
    "# on the other hand what may happen if... \n",
    "if tr_data.shape[1] < te_data.shape[1] : \n",
    "    # this means te_data have fewer follow-ups than tr_data. For this, patch it up with vectors of zero. \n",
    "    print('Test set [1] has fewer follow-ups than train set. Artificially curtailed excessive follow-ups to avoid critical failures. ')\n",
    "    te_data = te_data[:, range(tr_data.shape[1]), :]\n",
    "    te_data_mi = te_data_mi[:, range(tr_data_mi.shape[1]), :]\n",
    "\n",
    "if tr_data.shape[1] < tea_data.shape[1] : \n",
    "    \n",
    "    print('Test set [2] has fewer follow-ups than train set. Artificially curtailed excessive follow-ups to avoid critical failures. ')\n",
    "    tea_data = tea_data[:, range(tr_data.shape[1]), :]\n",
    "    tea_data_mi = tea_data_mi[:, range(tr_data_mi.shape[1]), :]\n",
    "\n",
    "pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "# A little treat: print name (in dict) of dataset\n",
    "def get_key(val):\n",
    "    for key, value in dataset_info.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"There is no such Key\"\n",
    "\n",
    "train_name = get_key(train_dir)\n",
    "test1_name = get_key(test_dir[0])\n",
    "test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : tf.nn.relu,\n",
    "                                'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                 }\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, mod_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6175a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_max(x): \n",
    "    # return indices of elements that equals to max(x)\n",
    "    max_x = max(x)\n",
    "    return list(x).index(max_x)\n",
    "\n",
    "eval_path = target_dir + '/eval'\n",
    "\n",
    "temporal_attention = np.loadtxt(eval_path + '/_longi_att__.txt',  dtype = 'float', delimiter = ',')\n",
    "rs = temporal_attention.sum(axis = 1)\n",
    "print('Temporal attention completeness: ' + str(np.mean(rs)))\n",
    "# every number is stored as np.float64, just to note\n",
    "\n",
    "# to find attention\n",
    "tr_J = []\n",
    "for i in range(temporal_attention.shape[0]): \n",
    "    tr_J.append(which_max(temporal_attention[i, :]) + 1)  # here we must plus 1 to balance out !!! \n",
    "\n",
    "# here, we also need how many times each patient was followed-up\n",
    "\n",
    "# this is done in tr_data\n",
    "\n",
    "# list(tr_data[id, :, 0].index(0)) + 1: this gives patient id == id's follow-up number\n",
    "# taking away the +1 adjustment gives the python's way of expressing follow-up number (eg. = 0 means the patient was followed only once)\n",
    "\n",
    "tr_FU = []\n",
    "for i in range(tr_data.shape[0]): \n",
    "    tr_FU.append(list(tr_data[i, :, 0]).index(0))\n",
    "\n",
    "# with this, we can determine whether patients were \"long-term\" or \"short-term\"\n",
    "# a patient is short-term is tr_FU[i] == tr_J[i] and long-term if tr_FU[i] > tr_J[i]\n",
    "# and we report \"error\" if tr_FU[i] < tr_J[i]\n",
    "\n",
    "tr_att_stat = []\n",
    "for i in range(len(tr_FU)): \n",
    "    if tr_FU[i] == tr_J[i]: \n",
    "        tr_att_stat.append('short-term')\n",
    "    elif tr_FU[i] > tr_J[i]: \n",
    "        tr_att_stat.append('long-term')\n",
    "    else: \n",
    "        tr_att_stat.append('error')\n",
    "        \n",
    "import pandas as pd\n",
    "print(pd.value_counts(tr_att_stat))\n",
    "\n",
    "\n",
    "# we do the same thing in test set... \n",
    "att_test = model.predict_att(te_data, te_data_mi, keep_prob = new_parser['keep_prob'])\n",
    "rs = att_test.sum(axis = 1)\n",
    "print('Temporal attention completeness: ' + str(np.mean(rs)))\n",
    "\n",
    "te_J = []\n",
    "for i in range(att_test.shape[0]): \n",
    "    te_J.append(which_max(att_test[i, :]) + 1)  # here we must plus 1 to balance out !!! \n",
    "\n",
    "\n",
    "te_FU = []\n",
    "for i in range(te_data.shape[0]): \n",
    "    te_FU.append(list(te_data[i, :, 0]).index(0))\n",
    "\n",
    "\n",
    "te_att_stat = []\n",
    "for i in range(len(te_FU)): \n",
    "    if te_FU[i] == te_J[i]: \n",
    "        te_att_stat.append('short-term')\n",
    "    elif te_FU[i] > te_J[i]: \n",
    "        te_att_stat.append('long-term')\n",
    "    else: \n",
    "        te_att_stat.append('error')\n",
    "        \n",
    "import pandas as pd\n",
    "print(pd.value_counts(te_att_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5677bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, new we want to remove these 'long-term' patients? \n",
    "tr_longterm_id = [tr_id[i] for i in range(len(tr_id)) if tr_FU[i] > tr_J[i]]\n",
    "te_longterm_id = [te_id[i] for i in range(len(te_id)) if te_FU[i] > te_J[i]]\n",
    "\n",
    "np.savetxt(eval_path + '/tr_longterm_id.txt', tr_longterm_id, delimiter = \",\")\n",
    "np.savetxt(eval_path + '/te_longterm_id.txt', te_longterm_id, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91c80b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2148, 6, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf81f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FU</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2148 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FU  J\n",
       "0      2  1\n",
       "1      2  1\n",
       "2      2  0\n",
       "3      2  1\n",
       "4      2  1\n",
       "...   .. ..\n",
       "2143   2  1\n",
       "2144   2  1\n",
       "2145   2  1\n",
       "2146   2  1\n",
       "2147   2  1\n",
       "\n",
       "[2148 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short-term    1739\n",
    "long-term      409\n",
    "dtype: int64\n",
    "short-term    1588\n",
    "long-term      432\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeabce77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python Model_evaluation.py PreCar 665087')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7efd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
