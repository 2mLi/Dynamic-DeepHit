{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e6edd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-mid classification: \n",
      "sensitivity:  0.5652173913043478\n",
      "specificity:  0.7181372549019608\n",
      "PPV:  0.02748414376321353\n",
      "NPV:  0.9915397631133672\n",
      "Low-mid classification: \n",
      "sensitivity:  0.7628865979381443\n",
      "specificity:  0.7957094100438811\n",
      "PPV:  0.15010141987829614\n",
      "NPV:  0.9861027190332327\n"
     ]
    }
   ],
   "source": [
    "risk_sub = risk_12[:, 3] # 1-13 HCC risk\n",
    "label_sub = tr_label[:, 0] * (tr_time[:, 0] <= 31)\n",
    "cutoff_1 = 0.1129\n",
    "cutoff_2 = 0.1542\n",
    "\n",
    "highrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] >= cutoff_1 and risk_sub[i] < cutoff_2]\n",
    "lowrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] < cutoff_1]\n",
    "TP = sum(highrisk_1)\n",
    "FP = len(highrisk_1) - TP\n",
    "FN = sum(lowrisk_1)\n",
    "TN = len(lowrisk_1) - FN\n",
    "print('Low-mid classification: ')\n",
    "print('sensitivity: ', str(TP/(TP + FN)))\n",
    "print('specificity: ', str(TN/(TN + FP)))\n",
    "print('PPV: ', str(TP/(TP + FP)))\n",
    "print('NPV: ', str(TN/(TN + FN)))\n",
    "\n",
    "highrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] >= cutoff_2]\n",
    "lowrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] < cutoff_2]\n",
    "TP = sum(highrisk_1)\n",
    "FP = len(highrisk_1) - TP\n",
    "FN = sum(lowrisk_1)\n",
    "TN = len(lowrisk_1) - FN\n",
    "print('Low-mid classification: ')\n",
    "print('sensitivity: ', str(TP/(TP + FN)))\n",
    "print('specificity: ', str(TN/(TN + FP)))\n",
    "print('PPV: ', str(TP/(TP + FP)))\n",
    "print('NPV: ', str(TN/(TN + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f7c4070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "status_1\n",
      "status_7\n",
      "status_13\n",
      "status_19\n",
      "status_1\n",
      "status_7\n",
      "status_13\n",
      "status_19\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = ['mod', 'PreCar', 'xx', '6', '10000']\n",
    "# cutoff_adjustment = adj\n",
    "\n",
    "_EPSILON = 1e-08\n",
    "\n",
    "#### <<< Warning suppression>>> ###\n",
    "# import warnings\n",
    "# warnings.filterwarnings('deprecated')\n",
    "#### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time as timepackage\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "\n",
    "from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "from utils_eval             import c_index, brier_score\n",
    "from utils_log              import save_logging, load_logging\n",
    "from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    \"\"\"\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    \"\"\"\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "    return risk_all\n",
    "\n",
    "## cmd args: \n",
    "# now only one argument is needed\n",
    "# this will be something like \"PreCar\"\n",
    "# and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the following codes read model training results plus needed data from Model_Training.py\n",
    "# and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "'''\n",
    "saver.restore(sess, sys.argv[1])\n",
    "with open(sys.argv[2]) as p: \n",
    "    params = json.load(p)\n",
    "'''\n",
    "\n",
    "# argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "# argv[2], if left empty, will choose the most recent log\n",
    "# if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "data_mode_name = sys.argv[1]\n",
    "\n",
    "if len(sys.argv) < 6: \n",
    "    # this means no argv[2] is given; we use the most recent log\n",
    "    # to do so, for now lets just use max argument\n",
    "    # firstly, take out all log.json documents\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    # logs is a list of all available logs; find the most recent one...\n",
    "    target_dir = data_mode_name + '/' + max(logs)\n",
    "    print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "else: \n",
    "    # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    matched = [i for i in logs if sys.argv[2] in i]\n",
    "    if len(matched) >= 2: \n",
    "        print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "        matched = max(matched)\n",
    "    target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "# read log\n",
    "with open(target_dir + '/' + '_log.json') as p: \n",
    "    params = json.load(p)\n",
    "mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "# print(type(params))\n",
    "new_parser = params['new_parser']\n",
    "dataset_info = params['dataset_info']\n",
    "evaluation_info = params['evaluation_info']\n",
    "model_configs = params['model_configs']\n",
    "eval_configs = params['eval_configs']\n",
    "time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "dirs = dataset_info\n",
    "test_dir = []\n",
    "data_mode = data_mode_name\n",
    "for key in list(dirs.keys()): \n",
    "    if key == data_mode: \n",
    "        train_dir = dirs[key]\n",
    "    else: \n",
    "        test_dir.append(dirs[key])\n",
    "\n",
    "(tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "#####\n",
    "\n",
    "# A little treat: print name (in dict) of dataset\n",
    "def get_key(val):\n",
    "    for key, value in dataset_info.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "\n",
    "    return \"There is no such Key\"\n",
    "\n",
    "train_name = get_key(train_dir)\n",
    "test1_name = get_key(test_dir[0])\n",
    "test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : tf.nn.relu,\n",
    "                                'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                 }\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, mod_dir)\n",
    "\n",
    "\n",
    "\n",
    "# By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "# Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "# here, we superseded eval_time and pred_time: \n",
    "\n",
    "pred_time_12 = 12\n",
    "pred_time_6 = 6\n",
    "eval_time = [1,7,13,19]\n",
    "\n",
    "\n",
    "risk_12 = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time_12], eval_time)\n",
    "risk_6 = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time_6], eval_time)\n",
    "\n",
    "risk_12 = risk_12[0][:, 0, :]\n",
    "risk_6 = risk_6[0][:, 0, :]\n",
    "\n",
    "# inflate it by 100 times\n",
    "risk_12 = 100 * risk_12\n",
    "risk_6 = 100 * risk_6\n",
    "\n",
    "# export csv\n",
    "# risk_12.savetxt('xtile_risk_12.txt', sep = '\\t', index = False)\n",
    "# risk_6.to_csv('xtile_risk_6.txt', sep = '\\t', index = False)\n",
    "# np.savetxt('xtile_risk_12.txt', risk_12)\n",
    "# np.savetxt('xtile_risk_6.txt', risk_6)\n",
    "\n",
    "\n",
    "col_header = eval_time\n",
    "risk_12_array = risk_12\n",
    "risk_6_array = risk_6\n",
    "risk_12 = pd.DataFrame(risk_12, index = range(risk_12.shape[0]), columns=col_header)\n",
    "risk_6 = pd.DataFrame(risk_6, index = range(risk_6.shape[0]), columns=col_header)\n",
    "\n",
    "# do not forget survival time and label\n",
    "\n",
    "new_label = []\n",
    "for i in tr_label: \n",
    "    if i == 1: \n",
    "        new_label.append(int(0))\n",
    "    elif i == 0: \n",
    "        new_label.append(int(1))\n",
    "\n",
    "# then, at each time window, transform new_label accordingly\n",
    "\n",
    "namespace = ['status_' + str(i) for i in eval_time]\n",
    "for tt in range(len(eval_time)): \n",
    "    print(namespace[tt])\n",
    "    t = eval_time[tt]\n",
    "    tv_label = []\n",
    "    for i in range(len(new_label)): \n",
    "        if new_label[i] == 0 and tr_time[i, 0] > t and tr_time[i, 0] <= t + 12: \n",
    "            tv_label.append(int(0))\n",
    "        else: \n",
    "            tv_label.append(int(1))\n",
    "    risk_12[namespace[tt]] = tv_label\n",
    "    \n",
    "namespace = ['status_' + str(i) for i in eval_time]\n",
    "for tt in range(len(eval_time)): \n",
    "    print(namespace[tt])\n",
    "    t = eval_time[tt]\n",
    "    tv_label = []\n",
    "    for i in range(len(new_label)): \n",
    "        if new_label[i] == 0 and tr_time[i, 0] > t and tr_time[i, 0] <= t + 6: \n",
    "            tv_label.append(int(0))\n",
    "        else: \n",
    "            tv_label.append(int(1))\n",
    "    risk_6[namespace[tt]] = tv_label\n",
    "    \n",
    "\n",
    "# risk_12 = risk_12.assign(status = new_label)\n",
    "# risk_6 = risk_6.assign(status = new_label)\n",
    "risk_12 = risk_12.assign(times = tr_time)\n",
    "risk_6 = risk_6.assign(times = tr_time)\n",
    "\n",
    "risk_12.to_csv('xtile_risk_12.txt', sep = '\\t', index = False)\n",
    "risk_6.to_csv('xtile_risk_6.txt', sep = '\\t', index = False)\n",
    "\n",
    "\n",
    "\n",
    "# on risk_12 and eval_time = 1, xtile has generated the following thresholds: 0.02 and 0.03\n",
    "# using these as cutting points: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d3edd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-mid classification: \n",
      "sensitivity:  0.65\n",
      "specificity:  0.6140350877192983\n",
      "PPV:  0.030162412993039442\n",
      "NPV:  0.9895833333333334\n",
      "Low-mid classification: \n",
      "sensitivity:  0.7938144329896907\n",
      "specificity:  0.5280351048269137\n",
      "PPV:  0.07368421052631578\n",
      "NPV:  0.9818676337262012\n"
     ]
    }
   ],
   "source": [
    "risk_sub = risk_6_array[:, 1]\n",
    "label_sub = tr_label[:, 0] * (tr_time[:, 0] <= 31)\n",
    "cutoff_1 = 4.39\n",
    "cutoff_2 = 5.07\n",
    "\n",
    "highrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] >= cutoff_1 and risk_sub[i] < cutoff_2]\n",
    "lowrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] < cutoff_1]\n",
    "TP = sum(highrisk_1)\n",
    "FP = len(highrisk_1) - TP\n",
    "FN = sum(lowrisk_1)\n",
    "TN = len(lowrisk_1) - FN\n",
    "print('Low-mid classification: ')\n",
    "print('sensitivity: ', str(TP/(TP + FN)))\n",
    "print('specificity: ', str(TN/(TN + FP)))\n",
    "print('PPV: ', str(TP/(TP + FP)))\n",
    "print('NPV: ', str(TN/(TN + FN)))\n",
    "\n",
    "highrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] >= cutoff_2]\n",
    "lowrisk_1 = [label_sub[i] for i in range(len(label_sub)) if risk_sub[i] < cutoff_2]\n",
    "TP = sum(highrisk_1)\n",
    "FP = len(highrisk_1) - TP\n",
    "FN = sum(lowrisk_1)\n",
    "TN = len(lowrisk_1) - FN\n",
    "print('Low-mid classification: ')\n",
    "print('sensitivity: ', str(TP/(TP + FN)))\n",
    "print('specificity: ', str(TN/(TN + FP)))\n",
    "print('PPV: ', str(TP/(TP + FP)))\n",
    "print('NPV: ', str(TN/(TN + FN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33383ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.66577804, 3.14186737, 5.27440161, ..., 4.00327742, 5.18431291,\n",
       "       4.62147668])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7f0e5ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11620\\1413455180.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrisk_6\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3928\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3929\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3930\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3932\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "risk_6.columns['7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cf84d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'7'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '7'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11620\\3924302430.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrisk_6\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '7'"
     ]
    }
   ],
   "source": [
    "risk_6['7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5717c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "========================================================\n",
      "========================================================\n",
      "========================================================\n",
      "=\n",
      "Used variables: Gender, CNV_score, Afp, Age, Alt, Alb, Plt, Tb, Inr, NF_CT, Fragment_CT, motif_CT, Comb_CT, score\n",
      "Log-transformed variables:  Afp, Alt, Alb, Plt, Tb, Inr, Comb_CT, score\n",
      "=\n",
      "========================================================\n"
     ]
    }
   ],
   "source": [
    "# can we make time-varying c-index etc, like they did in Tanner 2021? \n",
    "_EPSILON = 1e-08\n",
    "\n",
    "#### <<< Warning suppression>>> ###\n",
    "# import warnings\n",
    "# warnings.filterwarnings('deprecated')\n",
    "#### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time as timepackage\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import import_data as impt\n",
    "\n",
    "from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "from utils_eval             import c_index, brier_score\n",
    "from utils_log              import save_logging, load_logging\n",
    "from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "    \"\"\"\n",
    "        predictions based on the prediction time.\n",
    "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "    \"\"\"\n",
    "    new_data    = np.zeros(np.shape(data))\n",
    "    new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "    for i in range(np.shape(data)[0]):\n",
    "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "    return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "    \n",
    "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "    _, num_Event, num_Category = np.shape(pred)\n",
    "       \n",
    "    risk_all = {}\n",
    "    for k in range(num_Event):\n",
    "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "            \n",
    "    for p, p_time in enumerate(pred_time):\n",
    "        ### PREDICTION\n",
    "        pred_horizon = int(p_time)\n",
    "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "        for t, t_time in enumerate(eval_time):\n",
    "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "            \n",
    "            for k in range(num_Event):\n",
    "                risk_all[k][:, p, t] = risk[:, k]\n",
    "                \n",
    "    return risk_all\n",
    "\n",
    "## cmd args: \n",
    "# now only one argument is needed\n",
    "# this will be something like \"PreCar\"\n",
    "# and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### the following codes read model training results plus needed data from Model_Training.py\n",
    "# and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "'''\n",
    "saver.restore(sess, sys.argv[1])\n",
    "with open(sys.argv[2]) as p: \n",
    "    params = json.load(p)\n",
    "'''\n",
    "sys.argv = ['xx', 'PreCar', '191288']\n",
    "# argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "# argv[2], if left empty, will choose the most recent log\n",
    "# if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "data_mode_name = sys.argv[1]\n",
    "\n",
    "if len(sys.argv) < 3: \n",
    "    # this means no argv[2] is given; we use the most recent log\n",
    "    # to do so, for now lets just use max argument\n",
    "    # firstly, take out all log.json documents\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    # logs is a list of all available logs; find the most recent one...\n",
    "    target_dir = data_mode_name + '/' + max(logs)\n",
    "    print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "else: \n",
    "    # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "    logs = os.listdir(data_mode_name)\n",
    "    matched = [i for i in logs if sys.argv[2] in i]\n",
    "    if len(matched) >= 2: \n",
    "        print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "        matched = max(matched)\n",
    "    target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "# read log\n",
    "with open(target_dir + '/' + '_log.json') as p: \n",
    "    params = json.load(p)\n",
    "mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "# print(type(params))\n",
    "new_parser = params['new_parser']\n",
    "dataset_info = params['dataset_info']\n",
    "evaluation_info = params['evaluation_info']\n",
    "model_configs = params['model_configs']\n",
    "eval_configs = params['eval_configs']\n",
    "time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "dirs = dataset_info\n",
    "test_dir = []\n",
    "data_mode = data_mode_name\n",
    "for key in list(dirs.keys()): \n",
    "    if key == data_mode: \n",
    "        train_dir = dirs[key]\n",
    "    else: \n",
    "        test_dir.append(dirs[key])\n",
    "\n",
    "(tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "(tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "# check whether the dimension of tr_data, te_data and tea_data match\n",
    "# check for second dimension... \n",
    "if tr_data.shape[1] > te_data.shape[1] : \n",
    "    # this means te_data have fewer follow-ups than tr_data. For this, patch it up with vectors of zero. \n",
    "    print('Test set [1] has fewer follow-ups than train set. Artificially generated follow-ups have been attached. ')\n",
    "    k = tr_data.shape[1] - te_data.shape[1]\n",
    "    for i in range(k): \n",
    "        te_data = np.append(te_data, np.zeros(shape = (te_data.shape[0], 1, te_data.shape[2]), dtype = float), axis = 1) \n",
    "        te_data_mi = np.append(te_data_mi, np.zeros(shape = (te_data_mi.shape[0], 1, te_data_mi.shape[2]), dtype = float), axis = 1) \n",
    "\n",
    "if tr_data.shape[1] > tea_data.shape[1] : \n",
    "    \n",
    "    print('Test set [2] has fewer follow-ups than train set. Artificially generated follow-ups have been attached. ')\n",
    "    k = tr_data.shape[1] - tea_data.shape[1]\n",
    "    for i in range(k): \n",
    "        tea_data = np.append(tea_data, np.zeros(shape = (tea_data.shape[0], 1, tea_data.shape[2]), dtype = float), axis = 1) \n",
    "        tea_data_mi = np.append(tea_data_mi, np.zeros(shape = (tea_data_mi.shape[0], 1, tea_data_mi.shape[2]), dtype = float), axis = 1) \n",
    "\n",
    "# on the other hand what may happen if... \n",
    "if tr_data.shape[1] < te_data.shape[1] : \n",
    "    # this means te_data have fewer follow-ups than tr_data. For this, patch it up with vectors of zero. \n",
    "    print('Test set [1] has fewer follow-ups than train set. Artificially curtailed excessive follow-ups to avoid critical failures. ')\n",
    "    te_data = te_data[:, range(tr_data.shape[1]), :]\n",
    "    te_data_mi = te_data_mi[:, range(tr_data_mi.shape[1]), :]\n",
    "\n",
    "if tr_data.shape[1] < tea_data.shape[1] : \n",
    "    \n",
    "    print('Test set [2] has fewer follow-ups than train set. Artificially curtailed excessive follow-ups to avoid critical failures. ')\n",
    "    tea_data = tea_data[:, range(tr_data.shape[1]), :]\n",
    "    tea_data_mi = tea_data_mi[:, range(tr_data_mi.shape[1]), :]\n",
    "\n",
    "pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "_, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "#####\n",
    "\n",
    "# A little treat: print name (in dict) of dataset\n",
    "def get_key(val):\n",
    "    for key, value in dataset_info.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"There is no such Key\"\n",
    "\n",
    "train_name = get_key(train_dir)\n",
    "test1_name = get_key(test_dir[0])\n",
    "test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                'num_Event'     : num_Event,\n",
    "                                'num_Category'  : num_Category,\n",
    "                                'max_length'    : max_length }\n",
    "\n",
    "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                'RNN_type'          : new_parser['RNN_type'],\n",
    "                                'FC_active_fn'      : tf.nn.relu,\n",
    "                                'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                'reg_W'             : new_parser['reg_W'],\n",
    "                                'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                 }\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, mod_dir)\n",
    "\n",
    "# By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "# Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "# here since we're making plot, we want the plot to be somehow smoother\n",
    "eval_time = np.linspace(1, 25, 25)\n",
    "\n",
    "risk_all = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, pred_time, eval_time)\n",
    "\n",
    "for p, p_time in enumerate(pred_time):\n",
    "    pred_horizon = int(p_time)\n",
    "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "    for t, t_time in enumerate(eval_time):                \n",
    "        eval_horizon = int(t_time) + pred_horizon\n",
    "        for k in range(num_Event):\n",
    "            result1[k, t] = c_index(risk_all[k][:, p, t], tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            result2[k, t] = brier_score(risk_all[k][:, p, t], tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    \n",
    "    if p == 0:\n",
    "        final1, final2 = result1, result2\n",
    "    else:\n",
    "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "        \n",
    "        \n",
    "row_header = []\n",
    "for p_time in pred_time:\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "            \n",
    "col_header = []\n",
    "for t_time in eval_time:\n",
    "    col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "# c-index result\n",
    "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# brier-score result\n",
    "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "### print what variables are used\n",
    "feat_list = model_configs['bin_list'] + model_configs['cont_list']\n",
    "\n",
    "print('========================================================')\n",
    "print('========================================================')\n",
    "print('========================================================')\n",
    "print('=')\n",
    "print('Used variables: ' + \", \".join(feat_list))\n",
    "if len(model_configs['log_transform']) >= 1: \n",
    "    logged_var = [i for i in model_configs['log_transform'] if i in model_configs['cont_list']]\n",
    "    print('Log-transformed variables: ', \", \".join(logged_var))\n",
    "print('=')\n",
    "print('========================================================')\n",
    "\n",
    "\n",
    "# make dynamic graph\n",
    "# for now, let us say we're interested in 12-month c-index change\n",
    "tv_c_index_train = list(np.array(df1)[1, :])\n",
    "tv_brier_train = list(np.array(df2)[1, :])\n",
    "time = eval_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# here we need a step-like plot... \n",
    "# print(time)\n",
    "# print(tv_c_index)\n",
    "\n",
    "# plt.pyplot.step(x = time, y = tv_c_index)\n",
    "\n",
    "# do it on test sets\n",
    "\n",
    "risk_all = f_get_risk_predictions(sess, model, te_data, te_data_mi, pred_time, eval_time)\n",
    "\n",
    "for p, p_time in enumerate(pred_time):\n",
    "    pred_horizon = int(p_time)\n",
    "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "    for t, t_time in enumerate(eval_time):                \n",
    "        eval_horizon = int(t_time) + pred_horizon\n",
    "        for k in range(num_Event):\n",
    "            result1[k, t] = c_index(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            result2[k, t] = brier_score(risk_all[k][:, p, t], te_time, (te_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    \n",
    "    if p == 0:\n",
    "        final1, final2 = result1, result2\n",
    "    else:\n",
    "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "        \n",
    "        \n",
    "row_header = []\n",
    "for p_time in pred_time:\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "            \n",
    "col_header = []\n",
    "for t_time in eval_time:\n",
    "    col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "# c-index result\n",
    "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# brier-score result\n",
    "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "\n",
    "tv_c_index_test1 = list(np.array(df1)[1, :])\n",
    "tv_brier_test1 = list(np.array(df2)[1, :])\n",
    "\n",
    "# Test the Trained Network (in test 2)\n",
    "\n",
    "risk_all = f_get_risk_predictions(sess, model, tea_data, tea_data_mi, pred_time, eval_time)\n",
    "\n",
    "for p, p_time in enumerate(pred_time):\n",
    "    pred_horizon = int(p_time)\n",
    "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
    "\n",
    "    for t, t_time in enumerate(eval_time):                \n",
    "        eval_horizon = int(t_time) + pred_horizon\n",
    "        for k in range(num_Event):\n",
    "            result1[k, t] = c_index(risk_all[k][:, p, t], tea_time, (tea_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "            result2[k, t] = brier_score(risk_all[k][:, p, t], tea_time, (tea_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
    "    \n",
    "    if p == 0:\n",
    "        final1, final2 = result1, result2\n",
    "    else:\n",
    "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
    "        \n",
    "        \n",
    "row_header = []\n",
    "for p_time in pred_time:\n",
    "    for t in range(num_Event):\n",
    "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
    "            \n",
    "col_header = []\n",
    "for t_time in eval_time:\n",
    "    col_header.append('eval_time {}'.format(t_time))\n",
    "\n",
    "# c-index result\n",
    "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
    "\n",
    "# brier-score result\n",
    "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
    "\n",
    "tv_c_index_test2 = list(np.array(df1)[1, :])\n",
    "tv_brier_test2 = list(np.array(df2)[1, :])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626c8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.step(x = time, y = tv_c_index_train)\n",
    "plt.step(x = time, y = tv_c_index_test1)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "plt.step(x = time, y = tv_brier_train)\n",
    "plt.step(x = time, y = tv_brier_test1)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d707be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7051658966820663,\n",
       " 0.7019520479155634,\n",
       " 0.6957990704326064,\n",
       " 0.6939578119413657,\n",
       " 0.6925634608509117,\n",
       " 0.6972291741151233,\n",
       " 0.6963711119056132,\n",
       " 0.6934572756524848,\n",
       " 0.6898462638541294,\n",
       " 0.6897568823739721,\n",
       " 0.69465471684639,\n",
       " 0.6939584374776834,\n",
       " 0.6924944654716846,\n",
       " 0.6931371848889524,\n",
       " 0.6924298665143837,\n",
       " 0.6917874223713327,\n",
       " 0.6909308301805982,\n",
       " 0.6890748804340067,\n",
       " 0.6885752016560782,\n",
       " 0.6922335641373403,\n",
       " 0.6892176457991291,\n",
       " 0.6850417588692983,\n",
       " 0.686201727460918,\n",
       " 0.711792419159112,\n",
       " 0.7141302020129916]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_c_index_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722a180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
