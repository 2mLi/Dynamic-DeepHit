{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6499de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\utils_network.py:24: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\utils_network.py:29: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\class_DeepLongitudinal.py:20: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mod:303: RuntimeWarning: invalid value encountered in true_divide\n",
      "mod:297: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12172\\2005661300.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;31m# sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.95\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cutoff: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12172\\2005661300.py\u001b[0m in \u001b[0;36msensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals\u001b[1;34m(TP, FP, FN, TN, alpha)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mndtri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0msensitivity_point_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTP\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0msensitivity_confidence_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_proportion_confidence_interval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[0mspecificity_point_estimate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTN\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[0mspecificity_confidence_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_proportion_confidence_interval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTN\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12172\\2005661300.py\u001b[0m in \u001b[0;36m_proportion_confidence_interval\u001b[1;34m(r, n, z)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_proportion_confidence_interval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19, 25]\n",
    "adj_range = [0, 0, 0, 0, 0]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '6', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "    eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "    #####\n",
    "\n",
    "    # A little treat: print name (in dict) of dataset\n",
    "    def get_key(val):\n",
    "        for key, value in dataset_info.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "\n",
    "        return \"There is no such Key\"\n",
    "\n",
    "    train_name = get_key(train_dir)\n",
    "    test1_name = get_key(test_dir[0])\n",
    "    test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "    #####\n",
    "\n",
    "    input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                    'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                    'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category,\n",
    "                                    'max_length'    : max_length }\n",
    "\n",
    "    network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                    'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                    'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                    'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                    'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                    'RNN_type'          : new_parser['RNN_type'],\n",
    "                                    'FC_active_fn'      : tf.nn.relu,\n",
    "                                    'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                    'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                    'reg_W'             : new_parser['reg_W'],\n",
    "                                    'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                     }\n",
    "    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, mod_dir)\n",
    "    \n",
    "    \n",
    "\n",
    "    # By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "    # Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "    # here, we superseded eval_time and pred_time: \n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # then, new eval and pred time would be argument argv[2] and argv[3]\n",
    "        eval_time = float(sys.argv[2])\n",
    "        pred_time = float(sys.argv[3])\n",
    "        steps = int(sys.argv[4])\n",
    "    else: \n",
    "        eval_time = float(sys.argv[3])\n",
    "        pred_time = float(sys.argv[4])\n",
    "        steps = int(sys.argv[5])\n",
    "        \n",
    "        \n",
    "    risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])\n",
    "    risk = risk[0][:, 0, 0]\n",
    "\n",
    "    # we need: label, time\n",
    "    label = tr_label[:, 0]\n",
    "    time = tr_time[:, 0]\n",
    "    # true label: \n",
    "    label_tr = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    # we need a discretised scale from min(risk) to max(risk) in Train set\n",
    "    min_risk = min(risk)\n",
    "    max_risk = max(risk)\n",
    "    step = (max_risk - min_risk)/steps #step width\n",
    "    r = [min_risk + step * i for i in range(steps)]\n",
    "    r = r[1:len(r)]\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "    import stat_util    #Compute AUC with 95% confidence interval\n",
    "\n",
    "\n",
    "\n",
    "    from scipy.special import ndtri\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "    # okay, now use pred_time and eval_time to make something\n",
    "    # for this, we need a new step\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "\n",
    "    label = tr_label_sub[:, 0]\n",
    "    time = tr_time_sub[:, 0]\n",
    "    true_tr_label = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    tr_label_sub = true_tr_label\n",
    "\n",
    "    # now, risk\n",
    "    risk_sub = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])\n",
    "    risk_sub = list(risk_sub[0][:, 0, 0])\n",
    "\n",
    "    # okay given this risk, use log-rank...\n",
    "\n",
    "    risk_max = max(risk_sub)\n",
    "    risk_min = min(risk_sub)\n",
    "\n",
    "    # let us say steps = 100\n",
    "    steps = 100\n",
    "    step = (risk_max - risk_min)/steps\n",
    "\n",
    "    r = [risk_min + (i + 1) * step for i in range(steps - 1)]\n",
    "    # this should be a working example\n",
    "\n",
    "    youdenL = []\n",
    "\n",
    "    this_eval_time = eval_time\n",
    "    this_pred_time = pred_time\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    def _proportion_confidence_interval(r, n, z):\n",
    "        A = 2*r + z**2\n",
    "        B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "        C = 2*(n + z**2)\n",
    "        return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "    def sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        sensitivity_point_estimate = TP/(TP + FN)\n",
    "        sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "        specificity_point_estimate = TN/(TN + FP)\n",
    "        specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "\n",
    "        PPV_point_estimate = TP/(TP + FP)\n",
    "        PPV_CI = _proportion_confidence_interval(TP, TP+FP, z)\n",
    "        NPV_point_estimate = TN / (FN + TN)\n",
    "        NPV_CI = _proportion_confidence_interval(TN, FN + TN, z)\n",
    "        return sensitivity_point_estimate, specificity_point_estimate, PPV_point_estimate, NPV_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI\n",
    "\n",
    "    \n",
    "    for step in r: \n",
    "        # divide pops based on the step\n",
    "        grp1_idx = [i > step for i in risk_sub]\n",
    "        grp0_idx = [i <= step for i in risk_sub]\n",
    "        \n",
    "        grp1_data = tr_data_sub[grp1_idx, :, :]\n",
    "        grp1_time = tr_time_sub[grp1_idx]\n",
    "        grp1_label = tr_label_sub[grp1_idx]\n",
    "\n",
    "        # new label that is time-dynamic\n",
    "        # here, we should subset to include ONLY patients who survive beyond this_eval_time\n",
    "        grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "        grp1_label_new = np.zeros(len(grp1_label))\n",
    "        grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "        grp0_data = tr_data_sub[grp0_idx, :, :]\n",
    "        grp0_time = tr_time_sub[grp0_idx]\n",
    "        grp0_label = tr_label_sub[grp0_idx]\n",
    "\n",
    "        grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "        grp0_label_new = np.zeros(len(grp0_label))\n",
    "        grp0_label_new[grp0_label_idx] = 1\n",
    "        \n",
    "        # calculate sens and spec, then append to youdenL\n",
    "        risk = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])[0]\n",
    "        highrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] > step]\n",
    "        lowrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] <= step]\n",
    "\n",
    "        TP = sum(highrisk_label)\n",
    "        FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "        TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "        FN = sum(lowrisk_label)\n",
    "        res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "        (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "        \n",
    "        youdenL.append(sens + spec - 1)\n",
    "    \n",
    "\n",
    "    min_youden = max(youdenL)\n",
    "    min_idx = [i for i in range(len(r)) if youdenL[i] == min_youden][0]\n",
    "    cutoff_proto = r[min_idx]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Youden selection of best threshold\n",
    "    print('In train set... ')\n",
    "    cutoff = cutoff_proto + cutoff_adjustment * step\n",
    "\n",
    "    # use the cutoff to separate highrisk and lowrisk among test people\n",
    "\n",
    "    # comment this to use the true te_data\n",
    "    te_data = tr_data\n",
    "    te_data_mi = tr_data_mi_sub\n",
    "    te_label = tr_label_sub\n",
    "    te_time = tr_time_sub\n",
    "\n",
    "    risk = f_get_risk_predictions(sess, model, te_data, te_data_mi, [pred_time], [eval_time])\n",
    "    risk = list(risk[0][:, 0, 0])\n",
    "\n",
    "    # we need: label, time\n",
    "    label = te_label\n",
    "    # [:, 0]\n",
    "    time = te_time\n",
    "    # true label: \n",
    "    label_te = label * (time <= pred_time + eval_time)\n",
    "\n",
    "\n",
    "    highrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] > cutoff]\n",
    "    lowrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] <= cutoff]\n",
    "\n",
    "    TP = sum(highrisk_label)\n",
    "    FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "    TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "    FN = sum(lowrisk_label)\n",
    "\n",
    "    # sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "\n",
    "    res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "    print('Cutoff: ' + str(cutoff))\n",
    "\n",
    "    # unpack res\n",
    "    (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "    print('Positive N: ' + str(TP + FP))\n",
    "    print('Negative N: ' + str(TN + FN))\n",
    "    print('Current HCC: ' + str(TP + FN))\n",
    "    print('Current LC: ' + str(FP + TN))\n",
    "    print('sensitivity: ' + str(sens) + ' [' + str(sens_LB) + ', ' + str(sens_UB) + ']')\n",
    "    print('specificity: ' + str(spec) + ' [' + str(spec_LB) + ', ' + str(spec_UB) + ']')\n",
    "    print('PPV: ' + str(PPV) + ' [' + str(PPV_LB) + ', ' + str(PPV_UB) + ']')\n",
    "    print('NPV: ' + str(NPV) + ' [' + str(NPV_LB) + ', ' + str(NPV_UB) + ']')\n",
    "    \n",
    "     # before printing enrichment ratio, modify NPV if it equals zero\n",
    "    if 1-NPV == 0: \n",
    "        NPV = NPV - 0.0001\n",
    "    print('Enrichment ratio: ' + str(PPV/(1-NPV)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61615dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-18.83333333,  36.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  6.76666667,  36.        ,   1.64038214, ...,  15.6       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  6.36666667,  36.        ,   1.61992781, ...,  12.6       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  36.        ,   1.59395041, ...,  13.6       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]],\n",
       "\n",
       "       [[-11.43333333,  51.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [ -6.43333333,  51.        ,   1.62459156, ...,  19.1       ,\n",
       "           0.        ,   0.        ],\n",
       "        [ 12.23333333,  51.        ,   1.63052967, ...,  28.5       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  51.        ,   1.62828683, ...,  21.3       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]],\n",
       "\n",
       "       [[-17.26666667,  40.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  5.2       ,  40.        ,   1.7058638 , ...,  19.9       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  6.76666667,  40.        ,   1.68223545, ...,  16.3       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  40.        ,   1.66913094, ...,  13.4       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  4.46666667,  53.        ,   1.64147421, ...,  27.2       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  6.36666667,  53.        ,   1.59106472, ...,  24.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  53.        ,   1.57287172, ...,  21.3       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]],\n",
       "\n",
       "       [[  4.96666667,  52.        ,   1.59549633, ...,  27.6       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  6.76666667,  52.        ,   1.61278396, ...,  22.6       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  52.        ,   1.64345278, ...,  24.5       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]],\n",
       "\n",
       "       [[  6.8       ,  48.        ,   1.65801149, ...,   7.4       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  7.43333333,  48.        ,   1.68663636, ...,   8.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,  48.        ,   1.67209795, ...,   8.8       ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ],\n",
       "        [  0.        ,   0.        ,   0.        , ...,   0.        ,\n",
       "           0.        ,   0.        ]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ccccc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7028\\1644517719.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mtest_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mtr_x_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_x_dim_cont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_x_dim_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_mask1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_mask2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_mask3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_data_mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtr_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_feat_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_list_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_list_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cont_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_transform'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mte_x_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_x_dim_cont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_x_dim_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mte_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mte_mask1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_mask2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_mask3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mte_data_mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mte_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_feat_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_list_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcont_list_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cont_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_configs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_transform'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\import_data.py\u001b[0m in \u001b[0;36mimport_dataset\u001b[1;34m(path, bin_list_in, cont_list_in, log_list)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mpat_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mf_construct_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_org\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mf_construct_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_org_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[0mdata_mi\u001b[0m                  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\import_data.py\u001b[0m in \u001b[0;36mf_construct_dataset\u001b[1;34m(df, feat_list)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;31m# pat_info[i,5] = tmp['Patient'][0] # Patient ID as strings. necessary for internal/external validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeat_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2810\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2812\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2814\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   3407\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3408\u001b[0m         \"\"\"\n\u001b[1;32m-> 3409\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3410\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m         new_data = self._data.take(\n\u001b[1;32m-> 3395\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3396\u001b[0m         )\n\u001b[0;32m   3397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m         return self.reindex_indexer(\n\u001b[1;32m-> 1394\u001b[1;33m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m         )\n\u001b[0;32m   1396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m             \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m             new_blocks = [\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m             blknos = algos.take_1d(\n\u001b[1;32m-> 1315\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m             )\n\u001b[0;32m   1317\u001b[0m             blklocs = algos.take_1d(\n",
      "\u001b[1;32mF:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1660\u001b[0m         \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     )\n\u001b[1;32m-> 1662\u001b[1;33m     \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19, 25]\n",
    "adj_range = [0.41,0.46,0.45,0.48,0.19]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '12', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "    eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "    #####\n",
    "\n",
    "    # A little treat: print name (in dict) of dataset\n",
    "    def get_key(val):\n",
    "        for key, value in dataset_info.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "\n",
    "        return \"There is no such Key\"\n",
    "\n",
    "    train_name = get_key(train_dir)\n",
    "    test1_name = get_key(test_dir[0])\n",
    "    test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "    #####\n",
    "\n",
    "    input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                    'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                    'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category,\n",
    "                                    'max_length'    : max_length }\n",
    "\n",
    "    network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                    'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                    'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                    'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                    'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                    'RNN_type'          : new_parser['RNN_type'],\n",
    "                                    'FC_active_fn'      : tf.nn.relu,\n",
    "                                    'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                    'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                    'reg_W'             : new_parser['reg_W'],\n",
    "                                    'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                     }\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, mod_dir)\n",
    "\n",
    "    # By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "    # Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "    # here, we superseded eval_time and pred_time: \n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # then, new eval and pred time would be argument argv[2] and argv[3]\n",
    "        eval_time = float(sys.argv[2])\n",
    "        pred_time = float(sys.argv[3])\n",
    "        steps = int(sys.argv[4])\n",
    "    else: \n",
    "        eval_time = float(sys.argv[3])\n",
    "        pred_time = float(sys.argv[4])\n",
    "        steps = int(sys.argv[5])\n",
    "\n",
    "    \n",
    "    risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])\n",
    "    risk = risk[0][:, 0, 0]\n",
    "\n",
    "    # we need: label, time\n",
    "    label = tr_label[:, 0]\n",
    "    time = tr_time[:, 0]\n",
    "    # true label: \n",
    "    label_tr = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    # we need a discretised scale from min(risk) to max(risk) in Train set\n",
    "    min_risk = min(risk)\n",
    "    max_risk = max(risk)\n",
    "    step = (max_risk - min_risk)/steps #step width\n",
    "    r = [min_risk + step * i for i in range(steps)]\n",
    "    r = r[1:len(r)]\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "    import stat_util    #Compute AUC with 95% confidence interval\n",
    "\n",
    "\n",
    "\n",
    "    from scipy.special import ndtri\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "    # okay, now use pred_time and eval_time to make something\n",
    "    # for this, we need a new step\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "\n",
    "    label = tr_label_sub[:, 0]\n",
    "    time = tr_time_sub[:, 0]\n",
    "    true_tr_label = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    tr_label_sub = true_tr_label\n",
    "\n",
    "    # now, risk\n",
    "    risk_sub = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])\n",
    "    risk_sub = list(risk_sub[0][:, 0, 0])\n",
    "\n",
    "    # okay given this risk, use log-rank...\n",
    "\n",
    "    risk_max = max(risk_sub)\n",
    "    risk_min = min(risk_sub)\n",
    "\n",
    "    # let us say steps = 100\n",
    "    steps = 100\n",
    "    step = (risk_max - risk_min)/steps\n",
    "\n",
    "    r = [risk_min + (i + 1) * step for i in range(steps - 1)]\n",
    "    # this should be a working example\n",
    "\n",
    "    youdenL = []\n",
    "\n",
    "    this_eval_time = eval_time\n",
    "    this_pred_time = pred_time\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    def _proportion_confidence_interval(r, n, z):\n",
    "        A = 2*r + z**2\n",
    "        B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "        C = 2*(n + z**2)\n",
    "        return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "    def sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        sensitivity_point_estimate = TP/(TP + FN)\n",
    "        sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "        specificity_point_estimate = TN/(TN + FP)\n",
    "        specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "\n",
    "        PPV_point_estimate = TP/(TP + FP)\n",
    "        PPV_CI = _proportion_confidence_interval(TP, TP+FP, z)\n",
    "        NPV_point_estimate = TN / (FN + TN)\n",
    "        NPV_CI = _proportion_confidence_interval(TN, FN + TN, z)\n",
    "        return sensitivity_point_estimate, specificity_point_estimate, PPV_point_estimate, NPV_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI\n",
    "\n",
    "    \n",
    "    for step in r: \n",
    "        # divide pops based on the step\n",
    "        grp1_idx = [i > step for i in risk_sub]\n",
    "        grp0_idx = [i <= step for i in risk_sub]\n",
    "        \n",
    "        grp1_data = tr_data_sub[grp1_idx, :, :]\n",
    "        grp1_time = tr_time_sub[grp1_idx]\n",
    "        grp1_label = tr_label_sub[grp1_idx]\n",
    "\n",
    "        # new label that is time-dynamic\n",
    "        grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "        grp1_label_new = np.zeros(len(grp1_label))\n",
    "        grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "        grp0_data = tr_data_sub[grp0_idx, :, :]\n",
    "        grp0_time = tr_time_sub[grp0_idx]\n",
    "        grp0_label = tr_label_sub[grp0_idx]\n",
    "\n",
    "        grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "        grp0_label_new = np.zeros(len(grp0_label))\n",
    "        grp0_label_new[grp0_label_idx] = 1\n",
    "        \n",
    "        # calculate sens and spec, then append to youdenL\n",
    "        risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])[0]\n",
    "        highrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] > step]\n",
    "        lowrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] <= step]\n",
    "\n",
    "        TP = sum(highrisk_label)\n",
    "        FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "        TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "        FN = sum(lowrisk_label)\n",
    "        res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "        (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "        \n",
    "        youdenL.append(sens + spec - 1)\n",
    "    \n",
    "\n",
    "    min_youden = max(youdenL)\n",
    "    min_idx = [i for i in range(len(r)) if youdenL[i] == min_youden][0]\n",
    "    cutoff_proto = r[min_idx]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Youden selection of best threshold\n",
    "    print('In train set... ')\n",
    "    cutoff = cutoff_proto + cutoff_adjustment * step\n",
    "\n",
    "    # use the cutoff to separate highrisk and lowrisk among test people\n",
    "\n",
    "    # comment this to use the true te_data\n",
    "    # te_data = tr_data\n",
    "    # te_data_mi = tr_data_mi\n",
    "    # te_label = tr_label\n",
    "    # te_time = tr_time\n",
    "\n",
    "    risk = f_get_risk_predictions(sess, model, te_data, te_data_mi, [pred_time], [eval_time])\n",
    "    risk = list(risk[0][:, 0, 0])\n",
    "\n",
    "    # we need: label, time\n",
    "    label = te_label\n",
    "    time = te_time\n",
    "    # true label: \n",
    "    label_te = label * (time <= pred_time + eval_time)\n",
    "\n",
    "\n",
    "    highrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] > cutoff]\n",
    "    lowrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] <= cutoff]\n",
    "\n",
    "    TP = sum(highrisk_label)\n",
    "    FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "    TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "    FN = sum(lowrisk_label)\n",
    "\n",
    "    # sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "\n",
    "    res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "    print('Cutoff: ' + str(cutoff))\n",
    "\n",
    "    # unpack res\n",
    "    (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "    print('Positive N: ' + str(TP + FP))\n",
    "    print('Negative N: ' + str(TN + FN))\n",
    "    print('Current HCC: ' + str(TP + FN))\n",
    "    print('Current LC: ' + str(FP + TN))\n",
    "    print('sensitivity: ' + str(sens) + ' [' + str(sens_LB) + ', ' + str(sens_UB) + ']')\n",
    "    print('specificity: ' + str(spec) + ' [' + str(spec_LB) + ', ' + str(spec_UB) + ']')\n",
    "    print('PPV: ' + str(PPV) + ' [' + str(PPV_LB) + ', ' + str(PPV_UB) + ']')\n",
    "    print('NPV: ' + str(NPV) + ' [' + str(NPV_LB) + ', ' + str(NPV_UB) + ']')\n",
    "    \n",
    "     # before printing enrichment ratio, modify NPV if it equals zero\n",
    "    if 1-NPV == 0: \n",
    "        NPV = NPV - 0.0001\n",
    "    print('Enrichment ratio: ' + str(PPV/(1-NPV)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93021ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.018128324175952004\n",
      "Positive N: [261.]\n",
      "Negative N: [1887.]\n",
      "Current HCC: [16.]\n",
      "Current LC: [2132.]\n",
      "sensitivity: [0.6875] [[0.44404356], [0.85835356]]\n",
      "specificity: [0.88273921] [[0.86838902], [0.89571264]]\n",
      "PPV: [0.04214559] [[0.02369381], [0.0738795]]\n",
      "NPV: [0.99735029] [[0.99381205], [0.99886769]]\n",
      "Enrichment ratio: [15.90574713]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.14617371239792556\n",
      "Positive N: [4.]\n",
      "Negative N: [2113.]\n",
      "Current HCC: [54.]\n",
      "Current LC: [2063.]\n",
      "sensitivity: [0.] [[0.], [0.06641359]]\n",
      "specificity: [0.99806108] [[0.99502501], [0.99924574]]\n",
      "PPV: [0.] [[0.], [0.48989084]]\n",
      "NPV: [0.97444392] [[0.96680555], [0.98036033]]\n",
      "Enrichment ratio: [0.]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.2279086345296353\n",
      "Positive N: [12.]\n",
      "Negative N: [684.]\n",
      "Current HCC: [23.]\n",
      "Current LC: [673.]\n",
      "sensitivity: [0.17391304] [[0.06978654], [0.37137648]]\n",
      "specificity: [0.98811293] [[0.97672063], [0.99396459]]\n",
      "PPV: [0.33333333] [[0.13812009], [0.60937791]]\n",
      "NPV: [0.97222222] [[0.95702393], [0.98214598]]\n",
      "Enrichment ratio: [12.]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.2209775005105883\n",
      "Positive N: [9.]\n",
      "Negative N: [133.]\n",
      "Current HCC: [4.]\n",
      "Current LC: [138.]\n",
      "sensitivity: [0.] [[0.], [0.48989084]]\n",
      "specificity: [0.93478261] [[0.8807023], [0.96531269]]\n",
      "PPV: [0.] [[0.], [0.29914505]]\n",
      "NPV: [0.96992481] [[0.92522238], [0.98824347]]\n",
      "Enrichment ratio: [0.]\n"
     ]
    }
   ],
   "source": [
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19]\n",
    "adj_range = [+0.2, 0.45, 0.43, 0.23]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '6', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "    eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "    #####\n",
    "    \n",
    "    \n",
    "\n",
    "    # A little treat: print name (in dict) of dataset\n",
    "    def get_key(val):\n",
    "        for key, value in dataset_info.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "\n",
    "        return \"There is no such Key\"\n",
    "\n",
    "    train_name = get_key(train_dir)\n",
    "    test1_name = get_key(test_dir[0])\n",
    "    test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "    #####\n",
    "\n",
    "    input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                    'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                    'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category,\n",
    "                                    'max_length'    : max_length }\n",
    "\n",
    "    network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                    'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                    'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                    'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                    'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                    'RNN_type'          : new_parser['RNN_type'],\n",
    "                                    'FC_active_fn'      : tf.nn.relu,\n",
    "                                    'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                    'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                    'reg_W'             : new_parser['reg_W'],\n",
    "                                    'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                     }\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, mod_dir)\n",
    "\n",
    "    # By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "    # Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "    # here, we superseded eval_time and pred_time: \n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # then, new eval and pred time would be argument argv[2] and argv[3]\n",
    "        eval_time = float(sys.argv[2])\n",
    "        pred_time = float(sys.argv[3])\n",
    "        steps = int(sys.argv[4])\n",
    "    else: \n",
    "        eval_time = float(sys.argv[3])\n",
    "        pred_time = float(sys.argv[4])\n",
    "        steps = int(sys.argv[5])\n",
    "\n",
    "    # first, subset data!!! \n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data = tr_data[list(idx), :, :]\n",
    "    tr_data_mi = tr_data_mi[list(idx), :, :]\n",
    "    tr_time = tr_time[idx, :]\n",
    "    tr_label = tr_label[idx, :]\n",
    "    \n",
    "    \n",
    "    risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])\n",
    "    risk = risk[0][:, 0, 0]\n",
    "\n",
    "    # we need: label, time\n",
    "    label = tr_label[:, 0]\n",
    "    time = tr_time[:, 0]\n",
    "    # true label: \n",
    "    label_tr = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    # we need a discretised scale from min(risk) to max(risk) in Train set\n",
    "    min_risk = min(risk)\n",
    "    max_risk = max(risk)\n",
    "    step = (max_risk - min_risk)/steps #step width\n",
    "    r = [min_risk + step * i for i in range(steps)]\n",
    "    r = r[1:len(r)]\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "    import stat_util    #Compute AUC with 95% confidence interval\n",
    "\n",
    "\n",
    "\n",
    "    from scipy.special import ndtri\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "    # okay, now use pred_time and eval_time to make something\n",
    "    # for this, we need a new step\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "\n",
    "    label = tr_label_sub[:, 0]\n",
    "    time = tr_time_sub[:, 0]\n",
    "    true_tr_label = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    tr_label_sub = true_tr_label\n",
    "\n",
    "    # now, risk\n",
    "    risk_sub = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])\n",
    "    risk_sub = list(risk_sub[0][:, 0, 0])\n",
    "\n",
    "    # okay given this risk, use log-rank...\n",
    "\n",
    "    risk_max = max(risk_sub)\n",
    "    risk_min = min(risk_sub)\n",
    "\n",
    "    # let us say steps = 100\n",
    "    steps = 100\n",
    "    step = (risk_max - risk_min)/steps\n",
    "\n",
    "    r = [risk_min + (i + 1) * step for i in range(steps - 1)]\n",
    "    # this should be a working example\n",
    "\n",
    "    youdenL = []\n",
    "\n",
    "    this_eval_time = eval_time\n",
    "    this_pred_time = pred_time\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    def _proportion_confidence_interval(r, n, z):\n",
    "        A = 2*r + z**2\n",
    "        B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "        C = 2*(n + z**2)\n",
    "        return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "    def sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        sensitivity_point_estimate = TP/(TP + FN)\n",
    "        sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "        specificity_point_estimate = TN/(TN + FP)\n",
    "        specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "\n",
    "        PPV_point_estimate = TP/(TP + FP)\n",
    "        PPV_CI = _proportion_confidence_interval(TP, TP+FP, z)\n",
    "        NPV_point_estimate = TN / (FN + TN)\n",
    "        NPV_CI = _proportion_confidence_interval(TN, FN + TN, z)\n",
    "        return sensitivity_point_estimate, specificity_point_estimate, PPV_point_estimate, NPV_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI\n",
    "\n",
    "    \n",
    "    for step in r: \n",
    "        # divide pops based on the step\n",
    "        grp1_idx = [i > step for i in risk_sub]\n",
    "        grp0_idx = [i <= step for i in risk_sub]\n",
    "        \n",
    "        grp1_data = tr_data_sub[grp1_idx, :, :]\n",
    "        grp1_time = tr_time_sub[grp1_idx]\n",
    "        grp1_label = tr_label_sub[grp1_idx]\n",
    "\n",
    "        # new label that is time-dynamic\n",
    "        grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "        grp1_label_new = np.zeros(len(grp1_label))\n",
    "        grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "        grp0_data = tr_data_sub[grp0_idx, :, :]\n",
    "        grp0_time = tr_time_sub[grp0_idx]\n",
    "        grp0_label = tr_label_sub[grp0_idx]\n",
    "\n",
    "        grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "        grp0_label_new = np.zeros(len(grp0_label))\n",
    "        grp0_label_new[grp0_label_idx] = 1\n",
    "        \n",
    "        # calculate sens and spec, then append to youdenL\n",
    "        risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])[0]\n",
    "        highrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] > step]\n",
    "        lowrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] <= step]\n",
    "\n",
    "        TP = sum(highrisk_label)\n",
    "        FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "        TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "        FN = sum(lowrisk_label)\n",
    "        res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "        (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "        \n",
    "        youdenL.append(sens + spec - 1)\n",
    "    \n",
    "\n",
    "    min_youden = max(youdenL)\n",
    "    min_idx = [i for i in range(len(r)) if youdenL[i] == min_youden][0]\n",
    "    cutoff_proto = r[min_idx]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Youden selection of best threshold\n",
    "    print('In train set... ')\n",
    "    cutoff = cutoff_proto + cutoff_adjustment * step\n",
    "\n",
    "    # use the cutoff to separate highrisk and lowrisk among test people\n",
    "\n",
    "    # comment this to use the true te_data\n",
    "    te_data = tr_data\n",
    "    te_data_mi = tr_data_mi\n",
    "    te_label = tr_label\n",
    "    te_time = tr_time\n",
    "\n",
    "    risk = f_get_risk_predictions(sess, model, te_data, te_data_mi, [pred_time], [eval_time])\n",
    "    risk = list(risk[0][:, 0, 0])\n",
    "\n",
    "    # we need: label, time\n",
    "    label = te_label\n",
    "    time = te_time\n",
    "    # true label: \n",
    "    label_te = label * (time <= pred_time + eval_time)\n",
    "\n",
    "\n",
    "    highrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] > cutoff]\n",
    "    lowrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] <= cutoff]\n",
    "\n",
    "    TP = sum(highrisk_label)\n",
    "    FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "    TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "    FN = sum(lowrisk_label)\n",
    "\n",
    "    # sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "\n",
    "    res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "    print('Cutoff: ' + str(cutoff))\n",
    "\n",
    "    # unpack res\n",
    "    (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "    print('Positive N: ' + str(TP + FP))\n",
    "    print('Negative N: ' + str(TN + FN))\n",
    "    print('Current HCC: ' + str(TP + FN))\n",
    "    print('Current LC: ' + str(FP + TN))\n",
    "    print('sensitivity: ' + str(sens) + ' [' + str(sens_LB) + ', ' + str(sens_UB) + ']')\n",
    "    print('specificity: ' + str(spec) + ' [' + str(spec_LB) + ', ' + str(spec_UB) + ']')\n",
    "    print('PPV: ' + str(PPV) + ' [' + str(PPV_LB) + ', ' + str(PPV_UB) + ']')\n",
    "    print('NPV: ' + str(NPV) + ' [' + str(NPV_LB) + ', ' + str(NPV_UB) + ']')\n",
    "    \n",
    "     # before printing enrichment ratio, modify NPV if it equals zero\n",
    "    if 1-NPV == 0: \n",
    "        NPV = NPV - 0.0001\n",
    "    print('Enrichment ratio: ' + str(PPV/(1-NPV)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87ec52c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.018128324175952004\n",
      "Positive N: [255.]\n",
      "Negative N: [1765.]\n",
      "Current HCC: [10.]\n",
      "Current LC: [2010.]\n",
      "sensitivity: [0.1] [[0.01787621], [0.40415003]]\n",
      "specificity: [0.87363184] [[0.85838993], [0.88744833]]\n",
      "PPV: [0.00392157] [[0.00069259], [0.02187512]]\n",
      "NPV: [0.99490085] [[0.99033711], [0.997315]]\n",
      "Enrichment ratio: [0.76906318]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.14617371239792556\n",
      "Positive N: [7.]\n",
      "Negative N: [2013.]\n",
      "Current HCC: [34.]\n",
      "Current LC: [1986.]\n",
      "sensitivity: [0.05882353] [[0.01628266], [0.19093607]]\n",
      "specificity: [0.99748238] [[0.99411978], [0.99892416]]\n",
      "PPV: [0.28571429] [[0.08221892], [0.64106555]]\n",
      "NPV: [0.98410333] [[0.97764528], [0.98871725]]\n",
      "Enrichment ratio: [17.97321429]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.2279086345296353\n",
      "Positive N: [39.]\n",
      "Negative N: [1981.]\n",
      "Current HCC: [37.]\n",
      "Current LC: [1983.]\n",
      "sensitivity: [0.10810811] [[0.0428519], [0.24708532]]\n",
      "specificity: [0.98234997] [[0.97555283], [0.98728192]]\n",
      "PPV: [0.1025641] [[0.04061331], [0.23578854]]\n",
      "NPV: [0.98334175] [[0.97669854], [0.98811403]]\n",
      "Enrichment ratio: [6.15695416]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-03-28_02-10-21-191288_my_aMAP_model_with_CNVs/model\n",
      "In train set... \n",
      "Cutoff: 0.2209775005105883\n",
      "Positive N: [216.]\n",
      "Negative N: [1804.]\n",
      "Current HCC: [38.]\n",
      "Current LC: [1982.]\n",
      "sensitivity: [0.21052632] [[0.11074792], [0.36345779]]\n",
      "specificity: [0.8950555] [[0.88078994], [0.90779265]]\n",
      "PPV: [0.03703704] [[0.01888459], [0.0713689]]\n",
      "NPV: [0.98337029] [[0.97635957], [0.98832679]]\n",
      "Enrichment ratio: [2.22716049]\n"
     ]
    }
   ],
   "source": [
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19]\n",
    "adj_range = [+0.2, 0.45, 0.43, 0.23]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '6', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    pred_time = evaluation_info['pred_time'] # prediction time (in months)\n",
    "    eval_time = evaluation_info['eval_time'] # months evaluation time (for C-index and Brier-Score)\n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "    #####\n",
    "    \n",
    "    \n",
    "\n",
    "    # A little treat: print name (in dict) of dataset\n",
    "    def get_key(val):\n",
    "        for key, value in dataset_info.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "\n",
    "        return \"There is no such Key\"\n",
    "\n",
    "    train_name = get_key(train_dir)\n",
    "    test1_name = get_key(test_dir[0])\n",
    "    test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "    #####\n",
    "\n",
    "    input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                    'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                    'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category,\n",
    "                                    'max_length'    : max_length }\n",
    "\n",
    "    network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                    'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                    'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                    'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                    'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                    'RNN_type'          : new_parser['RNN_type'],\n",
    "                                    'FC_active_fn'      : tf.nn.relu,\n",
    "                                    'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                    'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                    'reg_W'             : new_parser['reg_W'],\n",
    "                                    'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                     }\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, mod_dir)\n",
    "\n",
    "    # By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "    # Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "    # here, we superseded eval_time and pred_time: \n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # then, new eval and pred time would be argument argv[2] and argv[3]\n",
    "        eval_time = float(sys.argv[2])\n",
    "        pred_time = float(sys.argv[3])\n",
    "        steps = int(sys.argv[4])\n",
    "    else: \n",
    "        eval_time = float(sys.argv[3])\n",
    "        pred_time = float(sys.argv[4])\n",
    "        steps = int(sys.argv[5])\n",
    "\n",
    "    # first, subset data!!! \n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data = tr_data[list(idx), :, :]\n",
    "    tr_data_mi = tr_data_mi[list(idx), :, :]\n",
    "    tr_time = tr_time[idx, :]\n",
    "    tr_label = tr_label[idx, :]\n",
    "    \n",
    "    \n",
    "    risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])\n",
    "    risk = risk[0][:, 0, 0]\n",
    "\n",
    "    # we need: label, time\n",
    "    label = tr_label[:, 0]\n",
    "    time = tr_time[:, 0]\n",
    "    # true label: \n",
    "    label_tr = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    # we need a discretised scale from min(risk) to max(risk) in Train set\n",
    "    min_risk = min(risk)\n",
    "    max_risk = max(risk)\n",
    "    step = (max_risk - min_risk)/steps #step width\n",
    "    r = [min_risk + step * i for i in range(steps)]\n",
    "    r = r[1:len(r)]\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "    import stat_util    #Compute AUC with 95% confidence interval\n",
    "\n",
    "\n",
    "\n",
    "    from scipy.special import ndtri\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "    # okay, now use pred_time and eval_time to make something\n",
    "    # for this, we need a new step\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "\n",
    "    label = tr_label_sub[:, 0]\n",
    "    time = tr_time_sub[:, 0]\n",
    "    true_tr_label = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    tr_label_sub = true_tr_label\n",
    "\n",
    "    # now, risk\n",
    "    risk_sub = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])\n",
    "    risk_sub = list(risk_sub[0][:, 0, 0])\n",
    "\n",
    "    # okay given this risk, use log-rank...\n",
    "\n",
    "    risk_max = max(risk_sub)\n",
    "    risk_min = min(risk_sub)\n",
    "\n",
    "    # let us say steps = 100\n",
    "    steps = 100\n",
    "    step = (risk_max - risk_min)/steps\n",
    "\n",
    "    r = [risk_min + (i + 1) * step for i in range(steps - 1)]\n",
    "    # this should be a working example\n",
    "\n",
    "    youdenL = []\n",
    "\n",
    "    this_eval_time = eval_time\n",
    "    this_pred_time = pred_time\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    def _proportion_confidence_interval(r, n, z):\n",
    "        A = 2*r + z**2\n",
    "        B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "        C = 2*(n + z**2)\n",
    "        return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "    def sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        sensitivity_point_estimate = TP/(TP + FN)\n",
    "        sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "        specificity_point_estimate = TN/(TN + FP)\n",
    "        specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "\n",
    "        PPV_point_estimate = TP/(TP + FP)\n",
    "        PPV_CI = _proportion_confidence_interval(TP, TP+FP, z)\n",
    "        NPV_point_estimate = TN / (FN + TN)\n",
    "        NPV_CI = _proportion_confidence_interval(TN, FN + TN, z)\n",
    "        return sensitivity_point_estimate, specificity_point_estimate, PPV_point_estimate, NPV_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI\n",
    "\n",
    "    \n",
    "    for step in r: \n",
    "        # divide pops based on the step\n",
    "        grp1_idx = [i > step for i in risk_sub]\n",
    "        grp0_idx = [i <= step for i in risk_sub]\n",
    "        \n",
    "        grp1_data = tr_data_sub[grp1_idx, :, :]\n",
    "        grp1_time = tr_time_sub[grp1_idx]\n",
    "        grp1_label = tr_label_sub[grp1_idx]\n",
    "\n",
    "        # new label that is time-dynamic\n",
    "        grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "        grp1_label_new = np.zeros(len(grp1_label))\n",
    "        grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "        grp0_data = tr_data_sub[grp0_idx, :, :]\n",
    "        grp0_time = tr_time_sub[grp0_idx]\n",
    "        grp0_label = tr_label_sub[grp0_idx]\n",
    "\n",
    "        grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "        grp0_label_new = np.zeros(len(grp0_label))\n",
    "        grp0_label_new[grp0_label_idx] = 1\n",
    "        \n",
    "        # calculate sens and spec, then append to youdenL\n",
    "        risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])[0]\n",
    "        highrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] > step]\n",
    "        lowrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] <= step]\n",
    "\n",
    "        TP = sum(highrisk_label)\n",
    "        FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "        TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "        FN = sum(lowrisk_label)\n",
    "        res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "        (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "        \n",
    "        youdenL.append(sens + spec - 1)\n",
    "    \n",
    "\n",
    "    min_youden = max(youdenL)\n",
    "    min_idx = [i for i in range(len(r)) if youdenL[i] == min_youden][0]\n",
    "    cutoff_proto = r[min_idx]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Youden selection of best threshold\n",
    "    print('In train set... ')\n",
    "    cutoff = cutoff_proto + cutoff_adjustment * step\n",
    "\n",
    "    # use the cutoff to separate highrisk and lowrisk among test people\n",
    "\n",
    "    # comment this to use the true te_data\n",
    "    #te_data = tr_data\n",
    "    #te_data_mi = tr_data_mi\n",
    "    #te_label = tr_label\n",
    "    #te_time = tr_time\n",
    "\n",
    "    risk = f_get_risk_predictions(sess, model, te_data, te_data_mi, [pred_time], [eval_time])\n",
    "    risk = list(risk[0][:, 0, 0])\n",
    "\n",
    "    # we need: label, time\n",
    "    label = te_label\n",
    "    time = te_time\n",
    "    # true label: \n",
    "    label_te = label * (time <= pred_time + eval_time)\n",
    "\n",
    "\n",
    "    highrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] > cutoff]\n",
    "    lowrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] <= cutoff]\n",
    "\n",
    "    TP = sum(highrisk_label)\n",
    "    FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "    TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "    FN = sum(lowrisk_label)\n",
    "\n",
    "    # sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "\n",
    "    res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "    print('Cutoff: ' + str(cutoff))\n",
    "\n",
    "    # unpack res\n",
    "    (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "    print('Positive N: ' + str(TP + FP))\n",
    "    print('Negative N: ' + str(TN + FN))\n",
    "    print('Current HCC: ' + str(TP + FN))\n",
    "    print('Current LC: ' + str(FP + TN))\n",
    "    print('sensitivity: ' + str(sens) + ' [' + str(sens_LB) + ', ' + str(sens_UB) + ']')\n",
    "    print('specificity: ' + str(spec) + ' [' + str(spec_LB) + ', ' + str(spec_UB) + ']')\n",
    "    print('PPV: ' + str(PPV) + ' [' + str(PPV_LB) + ', ' + str(PPV_UB) + ']')\n",
    "    print('NPV: ' + str(NPV) + ' [' + str(NPV_LB) + ', ' + str(NPV_UB) + ']')\n",
    "    \n",
    "     # before printing enrichment ratio, modify NPV if it equals zero\n",
    "    if 1-NPV == 0: \n",
    "        NPV = NPV - 0.0001\n",
    "    print('Enrichment ratio: ' + str(PPV/(1-NPV)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8ee53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_time = 25\n",
    "idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "tr_data = tr_data[list(idx), :, :]\n",
    "tr_data_mi = tr_data_mi[list(idx), :, :]\n",
    "tr_time = tr_time[idx, :]\n",
    "tr_label = tr_label[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2591c3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\utils_network.py:24: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\utils_network.py:29: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\Dynamic-DeepHit\\class_DeepLongitudinal.py:20: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\DDH\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.006082217395305633\n",
      "Landmark: 1.0\n",
      "Horizon: 6.0\n",
      "Positive N: [471.]\n",
      "Negative N: [1677.]\n",
      "Current HCC: [16.] ([14.], [2.])\n",
      "Current LC: [2132.] ([457.], [1675.])\n",
      "sensitivity: [0.875] [[0.63977173], [0.96502251]]\n",
      "specificity: [0.78564728] [[0.76772223], [0.80254482]]\n",
      "PPV: [0.02972399] [[0.01778706], [0.04926997]]\n",
      "NPV: [0.99880739] [[0.99566192], [0.99967288]]\n",
      "Enrichment ratio: [24.92356688]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.04786041259765625\n",
      "Landmark: 7.0\n",
      "Horizon: 6.0\n",
      "Positive N: [651.]\n",
      "Negative N: [1497.]\n",
      "Current HCC: [70.] ([47.], [23.])\n",
      "Current LC: [2078.] ([604.], [1474.])\n",
      "sensitivity: [0.67142857] [[0.55500936], [0.77001131]]\n",
      "specificity: [0.7093359] [[0.68944081], [0.72845845]]\n",
      "PPV: [0.07219662] [[0.05472513], [0.09468731]]\n",
      "NPV: [0.98463594] [[0.97705047], [0.98974052]]\n",
      "Enrichment ratio: [4.6990583]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.07912364244461059\n",
      "Landmark: 13.0\n",
      "Horizon: 6.0\n",
      "Positive N: [641.]\n",
      "Negative N: [1507.]\n",
      "Current HCC: [93.] ([62.], [31.])\n",
      "Current LC: [2055.] ([579.], [1476.])\n",
      "sensitivity: [0.66666667] [[0.5659347], [0.75417613]]\n",
      "specificity: [0.71824818] [[0.69840515], [0.73727677]]\n",
      "PPV: [0.09672387] [[0.07618627], [0.12206627]]\n",
      "NPV: [0.97942933] [[0.97094997], [0.9854707]]\n",
      "Enrichment ratio: [4.70202808]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.09443867951631546\n",
      "Landmark: 19.0\n",
      "Horizon: 6.0\n",
      "Positive N: [654.]\n",
      "Negative N: [1494.]\n",
      "Current HCC: [97.] ([65.], [32.])\n",
      "Current LC: [2051.] ([589.], [1462.])\n",
      "sensitivity: [0.67010309] [[0.57162727], [0.75561908]]\n",
      "specificity: [0.71282301] [[0.69285859], [0.7319917]]\n",
      "PPV: [0.09938838] [[0.07874586], [0.12470964]]\n",
      "NPV: [0.97858099] [[0.96991972], [0.98478746]]\n",
      "Enrichment ratio: [4.64019495]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.09663169831037521\n",
      "Landmark: 25.0\n",
      "Horizon: 6.0\n",
      "Positive N: [652.]\n",
      "Negative N: [1496.]\n",
      "Current HCC: [97.] ([65.], [32.])\n",
      "Current LC: [2051.] ([587.], [1464.])\n",
      "sensitivity: [0.67010309] [[0.57162727], [0.75561908]]\n",
      "specificity: [0.71379815] [[0.69385175], [0.73294517]]\n",
      "PPV: [0.09969325] [[0.07898983], [0.1250861]]\n",
      "NPV: [0.97860963] [[0.96995973], [0.98480785]]\n",
      "Enrichment ratio: [4.66065951]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.006082217395305633\n",
      "Landmark: 1.0\n",
      "Horizon: 6.0\n",
      "Positive N: [513.]\n",
      "Negative N: [1507.]\n",
      "Current HCC: [10.] ([6.], [4.])\n",
      "Current LC: [2010.] ([507.], [1503.])\n",
      "sensitivity: [0.6] [[0.31267377], [0.83181967]]\n",
      "specificity: [0.74776119] [[0.72831464], [0.76626253]]\n",
      "PPV: [0.01169591] [[0.00537106], [0.02527946]]\n",
      "NPV: [0.99734572] [[0.99319501], [0.99896733]]\n",
      "Enrichment ratio: [4.40643275]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.04786041259765625\n",
      "Landmark: 7.0\n",
      "Horizon: 6.0\n",
      "Positive N: [665.]\n",
      "Negative N: [1355.]\n",
      "Current HCC: [34.] ([20.], [14.])\n",
      "Current LC: [1986.] ([645.], [1341.])\n",
      "sensitivity: [0.58823529] [[0.42221594], [0.73634032]]\n",
      "specificity: [0.67522659] [[0.65430986], [0.69546675]]\n",
      "PPV: [0.03007519] [[0.01955204], [0.04599632]]\n",
      "NPV: [0.9896679] [[0.98273171], [0.99383549]]\n",
      "Enrichment ratio: [2.91084855]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.07912364244461059\n",
      "Landmark: 13.0\n",
      "Horizon: 6.0\n",
      "Positive N: [656.]\n",
      "Negative N: [1364.]\n",
      "Current HCC: [37.] ([22.], [15.])\n",
      "Current LC: [1983.] ([634.], [1349.])\n",
      "sensitivity: [0.59459459] [[0.43485965], [0.73653482]]\n",
      "specificity: [0.6802824] [[0.65942424], [0.70044343]]\n",
      "PPV: [0.03353659] [[0.02224974], [0.05025474]]\n",
      "NPV: [0.98900293] [[0.98193482], [0.9933244]]\n",
      "Enrichment ratio: [3.0495935]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n",
      "In train set... \n",
      "Cutoff: 0.09443867951631546\n",
      "Landmark: 19.0\n",
      "Horizon: 6.0\n",
      "Positive N: [668.]\n",
      "Negative N: [1352.]\n",
      "Current HCC: [38.] ([22.], [16.])\n",
      "Current LC: [1982.] ([646.], [1336.])\n",
      "sensitivity: [0.57894737] [[0.42192345], [0.72147499]]\n",
      "specificity: [0.6740666] [[0.65311174], [0.69434802]]\n",
      "PPV: [0.03293413] [[0.02184823], [0.04936122]]\n",
      "NPV: [0.98816568] [[0.98086262], [0.99270254]]\n",
      "Enrichment ratio: [2.78293413]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from PreCar/2022-04-25_01-23-17-994216_my_aMAP_model_with_CNVs_FS_aMAP/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train set... \n",
      "Cutoff: 0.09663169831037521\n",
      "Landmark: 25.0\n",
      "Horizon: 6.0\n",
      "Positive N: [666.]\n",
      "Negative N: [1354.]\n",
      "Current HCC: [39.] ([22.], [17.])\n",
      "Current LC: [1981.] ([644.], [1337.])\n",
      "sensitivity: [0.56410256] [[0.40975692], [0.70695246]]\n",
      "specificity: [0.67491166] [[0.65396361], [0.69518266]]\n",
      "PPV: [0.03303303] [[0.02191414], [0.04950793]]\n",
      "NPV: [0.98744461] [[0.97998479], [0.99214638]]\n",
      "Enrichment ratio: [2.63098393]\n"
     ]
    }
   ],
   "source": [
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19, 25]\n",
    "adj_range = [0, 0, 0, 0, 0]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '6', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "\n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "    #####\n",
    "    \n",
    "    \n",
    "\n",
    "    # A little treat: print name (in dict) of dataset\n",
    "    def get_key(val):\n",
    "        for key, value in dataset_info.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "\n",
    "        return \"There is no such Key\"\n",
    "\n",
    "    train_name = get_key(train_dir)\n",
    "    test1_name = get_key(test_dir[0])\n",
    "    test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "    #####\n",
    "\n",
    "    input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                    'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                    'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category,\n",
    "                                    'max_length'    : max_length }\n",
    "\n",
    "    network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                    'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                    'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                    'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                    'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                    'RNN_type'          : new_parser['RNN_type'],\n",
    "                                    'FC_active_fn'      : tf.nn.relu,\n",
    "                                    'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                    'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                    'reg_W'             : new_parser['reg_W'],\n",
    "                                    'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                     }\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, mod_dir)\n",
    "\n",
    "    # By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "    # Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "    # here, we superseded eval_time and pred_time: \n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # then, new eval and pred time would be argument argv[2] and argv[3]\n",
    "        eval_time = float(sys.argv[2])\n",
    "        pred_time = float(sys.argv[3])\n",
    "        steps = int(sys.argv[4])\n",
    "    else: \n",
    "        eval_time = float(sys.argv[3])\n",
    "        pred_time = float(sys.argv[4])\n",
    "        steps = int(sys.argv[5])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])\n",
    "    risk = risk[0][:, 0, 0]\n",
    "\n",
    "    # we need: label, time\n",
    "    label = tr_label[:, 0]\n",
    "    time = tr_time[:, 0]\n",
    "    # true label: \n",
    "    label_tr = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    # we need a discretised scale from min(risk) to max(risk) in Train set\n",
    "    min_risk = min(risk)\n",
    "    max_risk = max(risk)\n",
    "    step = (max_risk - min_risk)/steps #step width\n",
    "    r = [min_risk + step * i for i in range(steps)]\n",
    "    r = r[1:len(r)]\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "    import stat_util    #Compute AUC with 95% confidence interval\n",
    "\n",
    "\n",
    "\n",
    "    from scipy.special import ndtri\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "    # okay, now use pred_time and eval_time to make something\n",
    "    # for this, we need a new step\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "\n",
    "    label = tr_label_sub[:, 0]\n",
    "    time = tr_time_sub[:, 0]\n",
    "    true_tr_label = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    tr_label_sub = true_tr_label\n",
    "\n",
    "    # now, risk\n",
    "    risk_sub = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])\n",
    "    risk_sub = list(risk_sub[0][:, 0, 0])\n",
    "\n",
    "    # okay given this risk, use log-rank...\n",
    "\n",
    "    risk_max = max(risk_sub)\n",
    "    risk_min = min(risk_sub)\n",
    "\n",
    "    # let us say steps = 100\n",
    "    steps = 100\n",
    "    step = (risk_max - risk_min)/steps\n",
    "\n",
    "    r = [risk_min + (i + 1) * step for i in range(steps - 1)]\n",
    "    # this should be a working example\n",
    "\n",
    "    youdenL = []\n",
    "\n",
    "    this_eval_time = eval_time\n",
    "    this_pred_time = pred_time\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    def _proportion_confidence_interval(r, n, z):\n",
    "        A = 2*r + z**2\n",
    "        B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "        C = 2*(n + z**2)\n",
    "        return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "    def sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        sensitivity_point_estimate = TP/(TP + FN)\n",
    "        sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "        specificity_point_estimate = TN/(TN + FP)\n",
    "        specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "\n",
    "        PPV_point_estimate = TP/(TP + FP)\n",
    "        PPV_CI = _proportion_confidence_interval(TP, TP+FP, z)\n",
    "        NPV_point_estimate = TN / (FN + TN)\n",
    "        NPV_CI = _proportion_confidence_interval(TN, FN + TN, z)\n",
    "        return sensitivity_point_estimate, specificity_point_estimate, PPV_point_estimate, NPV_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI\n",
    "\n",
    "    \n",
    "    for step in r: \n",
    "        # divide pops based on the step\n",
    "        grp1_idx = [i > step for i in risk_sub]\n",
    "        grp0_idx = [i <= step for i in risk_sub]\n",
    "        \n",
    "        grp1_data = tr_data_sub[grp1_idx, :, :]\n",
    "        grp1_time = tr_time_sub[grp1_idx]\n",
    "        grp1_label = tr_label_sub[grp1_idx]\n",
    "\n",
    "        # new label that is time-dynamic\n",
    "        grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "        grp1_label_new = np.zeros(len(grp1_label))\n",
    "        grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "        grp0_data = tr_data_sub[grp0_idx, :, :]\n",
    "        grp0_time = tr_time_sub[grp0_idx]\n",
    "        grp0_label = tr_label_sub[grp0_idx]\n",
    "\n",
    "        grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "        grp0_label_new = np.zeros(len(grp0_label))\n",
    "        grp0_label_new[grp0_label_idx] = 1\n",
    "        \n",
    "        # calculate sens and spec, then append to youdenL\n",
    "        risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])[0]\n",
    "        highrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] > step]\n",
    "        lowrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] <= step]\n",
    "\n",
    "        TP = sum(highrisk_label)\n",
    "        FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "        TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "        FN = sum(lowrisk_label)\n",
    "        res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "        (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "        \n",
    "        youdenL.append(sens + spec - 1)\n",
    "    \n",
    "\n",
    "    min_youden = max(youdenL)\n",
    "    min_idx = [i for i in range(len(r)) if youdenL[i] == min_youden][0]\n",
    "    cutoff_proto = r[min_idx]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Youden selection of best threshold\n",
    "    print('In train set... ')\n",
    "    cutoff = cutoff_proto + cutoff_adjustment * step\n",
    "\n",
    "    # use the cutoff to separate highrisk and lowrisk among test people\n",
    "\n",
    "    # comment this to use the true te_data\n",
    "    te_data = tr_data\n",
    "    te_data_mi = tr_data_mi\n",
    "    te_label = tr_label\n",
    "    te_time = tr_time\n",
    "\n",
    "    risk = f_get_risk_predictions(sess, model, te_data, te_data_mi, [pred_time], [eval_time])\n",
    "    risk = list(risk[0][:, 0, 0])\n",
    "\n",
    "    # we need: label, time\n",
    "    label = te_label\n",
    "    time = te_time\n",
    "    # true label: \n",
    "    label_te = label * (time <= pred_time + eval_time)\n",
    "\n",
    "\n",
    "    highrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] > cutoff]\n",
    "    lowrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] <= cutoff]\n",
    "\n",
    "    TP = sum(highrisk_label)\n",
    "    FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "    TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "    FN = sum(lowrisk_label)\n",
    "\n",
    "    # sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "\n",
    "    res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "    print('Cutoff: ' + str(cutoff))\n",
    "\n",
    "    # unpack res\n",
    "    (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "    print('Landmark: ' + str(eval_time))\n",
    "    print('Horizon: ' + str(pred_time))\n",
    "    print('Positive N: ' + str(TP + FP))\n",
    "    print('Negative N: ' + str(TN + FN))\n",
    "    print('Current HCC: ' + str(TP + FN) + ' (' + str(TP) + ', ' + str(FN) + ')')\n",
    "    print('Current LC: ' + str(FP + TN) + ' (' + str(FP) + ', ' + str(TN) + ')')\n",
    "    print('sensitivity: ' + str(sens) + ' [' + str(sens_LB) + ', ' + str(sens_UB) + ']')\n",
    "    print('specificity: ' + str(spec) + ' [' + str(spec_LB) + ', ' + str(spec_UB) + ']')\n",
    "    print('PPV: ' + str(PPV) + ' [' + str(PPV_LB) + ', ' + str(PPV_UB) + ']')\n",
    "    print('NPV: ' + str(NPV) + ' [' + str(NPV_LB) + ', ' + str(NPV_UB) + ']')\n",
    "    \n",
    "     # before printing enrichment ratio, modify NPV if it equals zero\n",
    "    if 1-NPV == 0: \n",
    "        NPV = NPV - 0.0001\n",
    "    print('Enrichment ratio: ' + str(PPV/(1-NPV)))\n",
    "\n",
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19, 25]\n",
    "adj_range = [0, 0, 0, 0, 0]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '6', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    \n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "\n",
    "    #####\n",
    "    \n",
    "    \n",
    "\n",
    "    # A little treat: print name (in dict) of dataset\n",
    "    def get_key(val):\n",
    "        for key, value in dataset_info.items():\n",
    "             if val == value:\n",
    "                 return key\n",
    "\n",
    "        return \"There is no such Key\"\n",
    "\n",
    "    train_name = get_key(train_dir)\n",
    "    test1_name = get_key(test_dir[0])\n",
    "    test2_name = get_key(test_dir[1])\n",
    "\n",
    "\n",
    "    #####\n",
    "\n",
    "    input_dims                  = { 'x_dim'         : tr_x_dim,\n",
    "                                    'x_dim_cont'    : tr_x_dim_cont,\n",
    "                                    'x_dim_bin'     : tr_x_dim_bin,\n",
    "                                    'num_Event'     : num_Event,\n",
    "                                    'num_Category'  : num_Category,\n",
    "                                    'max_length'    : max_length }\n",
    "\n",
    "    network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
    "                                    'h_dim_FC'          : new_parser['h_dim_FC'],\n",
    "                                    'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
    "                                    'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
    "                                    'num_layers_CS'     : new_parser['num_layers_CS'],\n",
    "                                    'RNN_type'          : new_parser['RNN_type'],\n",
    "                                    'FC_active_fn'      : tf.nn.relu,\n",
    "                                    'RNN_active_fn'     : tf.nn.tanh,\n",
    "                                    'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
    "\n",
    "                                    'reg_W'             : new_parser['reg_W'],\n",
    "                                    'reg_W_out'         : float(new_parser['reg_W_out'])\n",
    "                                     }\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    model = Model_Longitudinal_Attention(sess, \"Dynamic-DeepHit\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, mod_dir)\n",
    "\n",
    "    # By default, at each landmark time and horizon, both c-index and Brier score will be computed\n",
    "    # Results will be printed, and saved in a _log.txt document\n",
    "\n",
    "    # here, we superseded eval_time and pred_time: \n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # then, new eval and pred time would be argument argv[2] and argv[3]\n",
    "        eval_time = float(sys.argv[2])\n",
    "        pred_time = float(sys.argv[3])\n",
    "        steps = int(sys.argv[4])\n",
    "    else: \n",
    "        eval_time = float(sys.argv[3])\n",
    "        pred_time = float(sys.argv[4])\n",
    "        steps = int(sys.argv[5])\n",
    "\n",
    "    \n",
    "    \n",
    "    risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])\n",
    "    risk = risk[0][:, 0, 0]\n",
    "\n",
    "    # we need: label, time\n",
    "    label = tr_label[:, 0]\n",
    "    time = tr_time[:, 0]\n",
    "    # true label: \n",
    "    label_tr = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    # we need a discretised scale from min(risk) to max(risk) in Train set\n",
    "    min_risk = min(risk)\n",
    "    max_risk = max(risk)\n",
    "    step = (max_risk - min_risk)/steps #step width\n",
    "    r = [min_risk + step * i for i in range(steps)]\n",
    "    r = r[1:len(r)]\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "    import stat_util    #Compute AUC with 95% confidence interval\n",
    "\n",
    "\n",
    "\n",
    "    from scipy.special import ndtri\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lifelines.statistics import logrank_test\n",
    "\n",
    "\n",
    "    # okay, now use pred_time and eval_time to make something\n",
    "    # for this, we need a new step\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "\n",
    "    label = tr_label_sub[:, 0]\n",
    "    time = tr_time_sub[:, 0]\n",
    "    true_tr_label = label * (time <= pred_time + eval_time)\n",
    "\n",
    "    tr_label_sub = true_tr_label\n",
    "\n",
    "    # now, risk\n",
    "    risk_sub = f_get_risk_predictions(sess, model, tr_data_sub, tr_data_mi_sub, [pred_time], [eval_time])\n",
    "    risk_sub = list(risk_sub[0][:, 0, 0])\n",
    "\n",
    "    # okay given this risk, use log-rank...\n",
    "\n",
    "    risk_max = max(risk_sub)\n",
    "    risk_min = min(risk_sub)\n",
    "\n",
    "    # let us say steps = 100\n",
    "    steps = 100\n",
    "    step = (risk_max - risk_min)/steps\n",
    "\n",
    "    r = [risk_min + (i + 1) * step for i in range(steps - 1)]\n",
    "    # this should be a working example\n",
    "\n",
    "    youdenL = []\n",
    "\n",
    "    this_eval_time = eval_time\n",
    "    this_pred_time = pred_time\n",
    "    from math import sqrt\n",
    "    from operator import add\n",
    "    def _proportion_confidence_interval(r, n, z):\n",
    "        A = 2*r + z**2\n",
    "        B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "        C = 2*(n + z**2)\n",
    "        return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "    def sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        sensitivity_point_estimate = TP/(TP + FN)\n",
    "        sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "        specificity_point_estimate = TN/(TN + FP)\n",
    "        specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "\n",
    "        PPV_point_estimate = TP/(TP + FP)\n",
    "        PPV_CI = _proportion_confidence_interval(TP, TP+FP, z)\n",
    "        NPV_point_estimate = TN / (FN + TN)\n",
    "        NPV_CI = _proportion_confidence_interval(TN, FN + TN, z)\n",
    "        return sensitivity_point_estimate, specificity_point_estimate, PPV_point_estimate, NPV_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI\n",
    "\n",
    "    \n",
    "    for step in r: \n",
    "        # divide pops based on the step\n",
    "        grp1_idx = [i > step for i in risk_sub]\n",
    "        grp0_idx = [i <= step for i in risk_sub]\n",
    "        \n",
    "        grp1_data = tr_data_sub[grp1_idx, :, :]\n",
    "        grp1_time = tr_time_sub[grp1_idx]\n",
    "        grp1_label = tr_label_sub[grp1_idx]\n",
    "\n",
    "        # new label that is time-dynamic\n",
    "        grp1_label_idx = [i for i in range(len(grp1_label)) if grp1_label[i] == 1 and grp1_time[i] < this_eval_time + this_pred_time]\n",
    "        grp1_label_new = np.zeros(len(grp1_label))\n",
    "        grp1_label_new[grp1_label_idx] = 1\n",
    "\n",
    "        grp0_data = tr_data_sub[grp0_idx, :, :]\n",
    "        grp0_time = tr_time_sub[grp0_idx]\n",
    "        grp0_label = tr_label_sub[grp0_idx]\n",
    "\n",
    "        grp0_label_idx = [i for i in range(len(grp0_label)) if grp0_label[i] == 1 and grp0_time[i] < this_eval_time + this_pred_time]\n",
    "        grp0_label_new = np.zeros(len(grp0_label))\n",
    "        grp0_label_new[grp0_label_idx] = 1\n",
    "        \n",
    "        # calculate sens and spec, then append to youdenL\n",
    "        risk = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, [pred_time], [eval_time])[0]\n",
    "        highrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] > step]\n",
    "        lowrisk_label = [label_tr[i] for i in range(len(label_tr)) if risk[i] <= step]\n",
    "\n",
    "        TP = sum(highrisk_label)\n",
    "        FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "        TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "        FN = sum(lowrisk_label)\n",
    "        res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "        (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "        \n",
    "        youdenL.append(sens + spec - 1)\n",
    "    \n",
    "\n",
    "    min_youden = max(youdenL)\n",
    "    min_idx = [i for i in range(len(r)) if youdenL[i] == min_youden][0]\n",
    "    cutoff_proto = r[min_idx]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Youden selection of best threshold\n",
    "    print('In train set... ')\n",
    "    cutoff = cutoff_proto + cutoff_adjustment * step\n",
    "\n",
    "    # use the cutoff to separate highrisk and lowrisk among test people\n",
    "\n",
    "    # comment this to use the true te_data\n",
    "    #te_data = tr_data\n",
    "    #te_data_mi = tr_data_mi\n",
    "    #te_label = tr_label\n",
    "    #te_time = tr_time\n",
    "\n",
    "    risk = f_get_risk_predictions(sess, model, te_data, te_data_mi, [pred_time], [eval_time])\n",
    "    risk = list(risk[0][:, 0, 0])\n",
    "\n",
    "    # we need: label, time\n",
    "    label = te_label\n",
    "    time = te_time\n",
    "    # true label: \n",
    "    label_te = label * (time <= pred_time + eval_time)\n",
    "\n",
    "\n",
    "    highrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] > cutoff]\n",
    "    lowrisk_label = [label_te[i] for i in range(len(label_te)) if risk[i] <= cutoff]\n",
    "\n",
    "    TP = sum(highrisk_label)\n",
    "    FP = len(highrisk_label) - sum(highrisk_label)\n",
    "\n",
    "    TN = len(lowrisk_label) - sum(lowrisk_label)\n",
    "    FN = sum(lowrisk_label)\n",
    "\n",
    "    # sensitivity_point_estimate, specificity_point_estimate, PPV, NPV, sensitivity_confidence_interval, specificity_confidence_interval, PPV_CI, NPV_CI = sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "\n",
    "    res= sensitivity_and_specificity_and_PPV_NPV_with_confidence_intervals(TP, FP, FN, TN, alpha=.95)\n",
    "    print('Cutoff: ' + str(cutoff))\n",
    "\n",
    "    # unpack res\n",
    "    (sens, spec, PPV, NPV, (sens_LB, sens_UB), (spec_LB, spec_UB), (PPV_LB, PPV_UB), (NPV_LB, NPV_UB)) = res\n",
    "    print('Landmark: ' + str(eval_time))\n",
    "    print('Horizon: ' + str(pred_time))\n",
    "    print('Positive N: ' + str(TP + FP))\n",
    "    print('Negative N: ' + str(TN + FN))\n",
    "    print('Current HCC: ' + str(TP + FN) + ' (' + str(TP) + ', ' + str(FN) + ')')\n",
    "    print('Current LC: ' + str(FP + TN) + ' (' + str(FP) + ', ' + str(TN) + ')')\n",
    "    print('sensitivity: ' + str(sens) + ' [' + str(sens_LB) + ', ' + str(sens_UB) + ']')\n",
    "    print('specificity: ' + str(spec) + ' [' + str(spec_LB) + ', ' + str(spec_UB) + ']')\n",
    "    print('PPV: ' + str(PPV) + ' [' + str(PPV_LB) + ', ' + str(PPV_UB) + ']')\n",
    "    print('NPV: ' + str(NPV) + ' [' + str(NPV_LB) + ', ' + str(NPV_UB) + ']')\n",
    "    \n",
    "     # before printing enrichment ratio, modify NPV if it equals zero\n",
    "    if 1-NPV == 0: \n",
    "        NPV = NPV - 0.0001\n",
    "    print('Enrichment ratio: ' + str(PPV/(1-NPV)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3104318",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the most recent _log.json by default, since no specification is given. \n",
      "At eval time: 1\n",
      "Total surviving N: 2148/2020\n",
      "HCC: [70.]/[34.]\n",
      "LC: [2078.]/[1986.]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "At eval time: 7\n",
      "Total surviving N: 2117/1959\n",
      "HCC: [77.]/[27.]\n",
      "LC: [2040.]/[1932.]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "At eval time: 13\n",
      "Total surviving N: 696/839\n",
      "HCC: [27.]/[4.]\n",
      "LC: [669.]/[835.]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "At eval time: 19\n",
      "Total surviving N: 142/147\n",
      "HCC: [4.]/[2.]\n",
      "LC: [138.]/[145.]\n",
      "Using the most recent _log.json by default, since no specification is given. \n",
      "At eval time: 25\n",
      "Total surviving N: 45/54\n",
      "HCC: [0.]/[1.]\n",
      "LC: [45.]/[53.]\n"
     ]
    }
   ],
   "source": [
    "# for loop our interested range\n",
    "eval_times = [1, 7, 13, 19, 25]\n",
    "adj_range = [0, 0, 0, 0, 0]\n",
    "for eval_time, adj in zip(eval_times, adj_range): \n",
    "    import sys\n",
    "\n",
    "    sys.argv = ['mod', 'PreCar', eval_time, '6', '10000']\n",
    "    cutoff_adjustment = adj\n",
    "\n",
    "    _EPSILON = 1e-08\n",
    "\n",
    "    #### <<< Warning suppression>>> ###\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings('deprecated')\n",
    "    #### This makes the resulting log a lot nicer BUT could produce errors in very, very rare and unexpected circumstances. \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    import os\n",
    "    import sys\n",
    "    import json\n",
    "    import time as timepackage\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import import_data as impt\n",
    "\n",
    "    from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
    "\n",
    "    from utils_eval             import c_index, brier_score\n",
    "    from utils_log              import save_logging, load_logging\n",
    "    from utils_helper           import f_get_minibatch, f_get_boosted_trainset\n",
    "\n",
    "\n",
    "\n",
    "    def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
    "        \"\"\"\n",
    "            predictions based on the prediction time.\n",
    "            create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
    "        \"\"\"\n",
    "        new_data    = np.zeros(np.shape(data))\n",
    "        new_data_mi = np.zeros(np.shape(data_mi))\n",
    "\n",
    "        meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
    "\n",
    "        for i in range(np.shape(data)[0]):\n",
    "            last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
    "\n",
    "            new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
    "            new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
    "\n",
    "        return model.predict(new_data, new_data_mi)\n",
    "\n",
    "\n",
    "    def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
    "\n",
    "        pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
    "        _, num_Event, num_Category = np.shape(pred)\n",
    "\n",
    "        risk_all = {}\n",
    "        for k in range(num_Event):\n",
    "            risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
    "\n",
    "        for p, p_time in enumerate(pred_time):\n",
    "            ### PREDICTION\n",
    "            pred_horizon = int(p_time)\n",
    "            pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
    "\n",
    "\n",
    "            for t, t_time in enumerate(eval_time):\n",
    "                eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
    "\n",
    "                # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
    "                risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
    "                risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
    "\n",
    "                for k in range(num_Event):\n",
    "                    risk_all[k][:, p, t] = risk[:, k]\n",
    "\n",
    "        return risk_all\n",
    "\n",
    "    ## cmd args: \n",
    "    # now only one argument is needed\n",
    "    # this will be something like \"PreCar\"\n",
    "    # and the machine will know to find all relevant materials from the \"PreCar\" directory\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### the following codes read model training results plus needed data from Model_Training.py\n",
    "    # and theoretically can be used to re-construct everything needed? \n",
    "\n",
    "    '''\n",
    "    saver.restore(sess, sys.argv[1])\n",
    "    with open(sys.argv[2]) as p: \n",
    "        params = json.load(p)\n",
    "    '''\n",
    "\n",
    "    # argv[1] is the data_mode: eg if PreCar, the program will read it from the PreCar file\n",
    "    # argv[2], if left empty, will choose the most recent log\n",
    "    # if argv[2] is specified, will use the string to find relevant log\n",
    "\n",
    "    data_mode_name = sys.argv[1]\n",
    "\n",
    "    if len(sys.argv) < 6: \n",
    "        # this means no argv[2] is given; we use the most recent log\n",
    "        # to do so, for now lets just use max argument\n",
    "        # firstly, take out all log.json documents\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        # logs is a list of all available logs; find the most recent one...\n",
    "        target_dir = data_mode_name + '/' + max(logs)\n",
    "        print('Using the most recent _log.json by default, since no specification is given. ')\n",
    "    else: \n",
    "        # assume that argv[2] has specified a keyword, use the keyword to identify logs\n",
    "        logs = os.listdir(data_mode_name)\n",
    "        matched = [i for i in logs if sys.argv[2] in i]\n",
    "        if len(matched) >= 2: \n",
    "            print('Warning: more than one log is matched with the keyword and the most recent one will be used. ')\n",
    "            matched = max(matched)\n",
    "        target_dir = data_mode_name + '/' + matched[0]\n",
    "\n",
    "\n",
    "    # read log\n",
    "    with open(target_dir + '/' + '_log.json') as p: \n",
    "        params = json.load(p)\n",
    "    mod_dir = target_dir + '/' + 'model'\n",
    "\n",
    "    # print(type(params))\n",
    "    new_parser = params['new_parser']\n",
    "    dataset_info = params['dataset_info']\n",
    "    evaluation_info = params['evaluation_info']\n",
    "    model_configs = params['model_configs']\n",
    "    eval_configs = params['eval_configs']\n",
    "    time_tag = params['new_parser']['time_tag']\n",
    "\n",
    "    dirs = dataset_info\n",
    "    test_dir = []\n",
    "    data_mode = data_mode_name\n",
    "    for key in list(dirs.keys()): \n",
    "        if key == data_mode: \n",
    "            train_dir = dirs[key]\n",
    "        else: \n",
    "            test_dir.append(dirs[key])\n",
    "\n",
    "    (tr_x_dim, tr_x_dim_cont, tr_x_dim_bin), (tr_data, tr_time, tr_label), (tr_mask1, tr_mask2, tr_mask3), (tr_data_mi), (tr_id), tr_feat_list = impt.import_dataset(path = train_dir, bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (te_x_dim, te_x_dim_cont, te_x_dim_bin), (te_data, te_time, te_label), (te_mask1, te_mask2, te_mask3), (te_data_mi), (te_id), te_feat_list = impt.import_dataset(path = test_dir[0], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "    (tea_x_dim, tea_x_dim_cont, tea_x_dim_bin), (tea_data, tea_time, tea_label), (tea_mask1, tea_mask2, tea_mask3), (tea_data_mi), (tea_id), tea_feat_list = impt.import_dataset(path = test_dir[1], bin_list_in = model_configs['bin_list'], cont_list_in = model_configs['cont_list'], log_list = model_configs['log_transform'])\n",
    "\n",
    "\n",
    "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
    "\n",
    "    max_length                  = np.shape(tr_data)[1]\n",
    "    \n",
    "    # no of HCC/LC at window\n",
    "    # first, subset data: only those surviving past eval_time\n",
    "    idx = [i > eval_time for i in list(tr_time[:, 0])]\n",
    "    tr_data_sub = tr_data[list(idx), :, :]\n",
    "    tr_data_mi_sub = tr_data_mi[list(idx), :, :]\n",
    "    tr_time_sub = tr_time[idx, :]\n",
    "    tr_label_sub = tr_label[idx, :]\n",
    "    \n",
    "    idxt = [i > eval_time for i in list(te_time[:, 0])]\n",
    "    te_data_sub = te_data[list(idxt), :, :]\n",
    "    te_data_mi_sub = te_data_mi[list(idxt), :, :]\n",
    "    te_time_sub = te_time[idxt, :]\n",
    "    te_label_sub = te_label[idxt, :]\n",
    "    \n",
    "    # then, count how many has their events during [eval_time, eval_time + pred_time]\n",
    "    pred_time = 12\n",
    "    \n",
    "    true_label_tr = tr_label_sub * (tr_time_sub <= pred_time + eval_time)\n",
    "    true_label_te = te_label_sub * (te_time_sub <= pred_time + eval_time)\n",
    "    # cat total patient\n",
    "    print('At eval time: ' + str(eval_time))\n",
    "    print('Total surviving N: ' + str(len(true_label_tr)) + '/' + str(len(true_label_te)))\n",
    "    # cat HCC\n",
    "    print('HCC: ' + str(sum(true_label_tr)) + '/' + str(sum(true_label_te)))\n",
    "    # LC\n",
    "    print('LC: ' + str(len(true_label_tr) - sum(true_label_tr)) + '/' + str(len(true_label_te) - sum(true_label_te)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8547bc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i > 23 for i in list(tr_time[:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1926c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
